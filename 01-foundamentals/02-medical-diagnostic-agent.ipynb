{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure AI Agent Service Diagnostic Demo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Necessary Libraries\n",
    "In this cell, we import all the libraries and modules required for the project.\n",
    "This includes Azure AI SDKs, Gradio for UI, and custom functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment and authentication OK\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import uuid\n",
    "from datetime import datetime as pydatetime\n",
    "from typing import Any, List, Dict\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# (Optional) Gradio app for UI\n",
    "import gradio as gr\n",
    "from gradio import ChatMessage\n",
    "import base64\n",
    "\n",
    "# Azure AI Projects\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "from azure.ai.projects import AIProjectClient\n",
    "import azure.ai.agents as agentslib\n",
    "import azure.ai.projects as projectlib\n",
    "from azure.ai.agents.models import (\n",
    "    AgentEventHandler,\n",
    "    RunStep,\n",
    "    RunStepDeltaChunk,\n",
    "    ThreadMessage,\n",
    "    ThreadRun,\n",
    "    MessageDeltaChunk,\n",
    "    BingGroundingTool,\n",
    "    FilePurpose,\n",
    "    FileSearchTool,\n",
    "    FunctionTool,\n",
    "    ToolSet,\n",
    "    VectorStore,\n",
    "    AzureAISearchTool,\n",
    "    CodeInterpreterTool,\n",
    "    MessageDeltaTextContent,\n",
    "    MessageDeltaImageFileContent,\n",
    "    MessageDeltaTextContentObject,\n",
    "    MessageDeltaTextUrlCitationAnnotation,\n",
    "    MessageRole,\n",
    "    AgentThread,\n",
    "    MessageTextContent,\n",
    "    AgentsNamedToolChoice,\n",
    "    AgentsToolChoiceOptionMode,\n",
    "    AgentsNamedToolChoiceType,\n",
    ")\n",
    "\n",
    "# Your custom Python functions (for \"fetch_datetime\", etc.)\n",
    "from utils.enterprise_functions import enterprise_fns\n",
    "\n",
    "load_dotenv(dotenv_path=\".env\", override=True)\n",
    "\n",
    "from utils.fdyauth import AuthHelper\n",
    "settings = AuthHelper.load_settings()\n",
    "credential = AuthHelper.test_credential()\n",
    "\n",
    "if credential:\n",
    "    print('Environment and authentication OK')\n",
    "else:\n",
    "    print(\"please login first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Client and Load Azure AI Foundry\n",
    "Here, we initialize the Azure AI client using DefaultAzureCredential.\n",
    "This allows us to authenticate and connect to the Azure AI service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project_client api version: 2025-05-15-preview\n",
      "azure-ai-agents version: 1.1.0b3\n",
      "azure-ai-projects version: 1.0.0b12\n"
     ]
    }
   ],
   "source": [
    "# new AI Foundry Project resource endpoint / old azure ai services endpoint from the hub/project\n",
    "project_client = AIProjectClient(\n",
    "    credential=credential,\n",
    "    endpoint=settings.project_endpoint,\n",
    "    # api_version=os.environ[\"PROJECT_API_VERSION\"]\n",
    ")\n",
    "print(\"project_client api version:\", project_client._config.api_version)\n",
    "print(f\"azure-ai-agents version: {agentslib.__version__}\")\n",
    "print(f\"azure-ai-projects version: {projectlib.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up Tools (BingGroundingTool, FileSearchTool)\n",
    "In this step, we configure tools such as `BingGroundingTool` and `FileSearchTool`.\n",
    "We check for existing connections and create or reuse vector stores for document search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "If you see the following cell has error:\n",
    "```\n",
    "AzureCliCredential: Please run 'az login' to set up an account\n",
    "```\n",
    "\n",
    "relogin from powershell\n",
    "```powershell\n",
    "az logout\n",
    "az account clear\n",
    "az login --tenant 00000000-0000-0000-0000-000000000000\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reusing vector store > lung-treatment-vector-store (id: vs_kvDdrd5AJnCrmuRFP4fJkJWp)\n"
     ]
    }
   ],
   "source": [
    "# try:\n",
    "#     bing_connection = project_client.connections.get(name=os.environ[\"BING_CONNECTION_NAME\"])\n",
    "#     # print(f\"{bing_connection}\")\n",
    "#     conn_id = bing_connection.id\n",
    "#     bing_tool = BingGroundingTool(connection_id=conn_id)\n",
    "#     print(\"bing > connected\")\n",
    "# except Exception:\n",
    "#     bing_tool = None\n",
    "#     print(\"bing failed > no connection found or permission issue\")\n",
    "\n",
    "## need to be wrapped inside the agents_client, close agents_client if done\n",
    "FOLDER_NAME = \"treatment-data\"\n",
    "VECTOR_STORE_NAME = \"lung-treatment-vector-store\"\n",
    "\n",
    "# project_client.agents return the AgentsClient\n",
    "all_vector_stores: List[VectorStore] = project_client.agents.vector_stores.list()\n",
    "\n",
    "existing_vector_store = next(\n",
    "    (store for store in all_vector_stores if store.name == VECTOR_STORE_NAME),\n",
    "    None\n",
    ")\n",
    "\n",
    "vector_store_id = None\n",
    "if existing_vector_store:\n",
    "    vector_store_id = existing_vector_store.id\n",
    "    print(f\"reusing vector store > {existing_vector_store.name} (id: {existing_vector_store.id})\")\n",
    "else:\n",
    "    # If you have local docs to upload\n",
    "    import os\n",
    "    if os.path.isdir(FOLDER_NAME):\n",
    "        file_ids = []\n",
    "        for file_name in os.listdir(FOLDER_NAME):\n",
    "            file_path = os.path.join(FOLDER_NAME, file_name)\n",
    "            if os.path.isfile(file_path):\n",
    "                print(f\"uploading > {file_name}\")\n",
    "                uploaded_file = project_client.agents.files.upload_and_poll(\n",
    "                    file_path=file_path,\n",
    "                    purpose=FilePurpose.AGENTS\n",
    "                )\n",
    "                file_ids.append(uploaded_file.id)\n",
    "\n",
    "        if file_ids:\n",
    "            print(f\"creating vector store > from {len(file_ids)} files.\")\n",
    "            vector_store = project_client.agents.vector_stores.create_and_poll(\n",
    "                file_ids=file_ids,\n",
    "                name=VECTOR_STORE_NAME\n",
    "            )\n",
    "            vector_store_id = vector_store.id\n",
    "            print(f\"created > {vector_store.name} (id: {vector_store_id})\")\n",
    "\n",
    "file_search_tool = None\n",
    "if vector_store_id:\n",
    "    file_search_tool = FileSearchTool(vector_store_ids=[vector_store_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up Tools (Coding Interpreter)\n",
    "In this step, we configure tools such as `CodeInterpreterTool`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file already uploaded > lung_cancer_patients_symptoms_2025.csv (id: assistant-GJ6hZx35ajvkaqjsjzrbTB)\n",
      "file already uploaded > lung_cancer_patient_clinical_features.csv (id: assistant-5ri9uw9EoDCqkYrv5iZvep)\n"
     ]
    }
   ],
   "source": [
    "# Create Code Interpreter\n",
    "DATA_FOLDER_NAME = \"medicare-data\"\n",
    "uploaded_files_list = project_client.agents.files.list(purpose=FilePurpose.AGENTS).data\n",
    "\n",
    "# dictionary of uploaded files list with key file.filename, value file.id\n",
    "uploaded_files_dict = {file.filename: file.id for file in uploaded_files_list}\n",
    "\n",
    "data_file_ids = []\n",
    "if os.path.isdir(DATA_FOLDER_NAME):    \n",
    "    for file_name in os.listdir(DATA_FOLDER_NAME):\n",
    "        file_path = os.path.join(DATA_FOLDER_NAME, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            # test file_name exists as key in dictionary uploaded_files_dict\n",
    "            if file_name in uploaded_files_dict:\n",
    "                data_file_id = uploaded_files_dict[file_name]\n",
    "                data_file_ids.append(data_file_id)\n",
    "                print(f\"file already uploaded > {file_name} (id: {data_file_id})\")\n",
    "            else:\n",
    "                print(f\"uploading > {file_name}\")\n",
    "                uploaded_file = project_client.agents.files.upload_and_poll(\n",
    "                    file_path=file_path,\n",
    "                    purpose=FilePurpose.AGENTS\n",
    "                )\n",
    "                data_file_ids.append(uploaded_file.id)\n",
    "\n",
    "if len(data_file_ids) > 0:\n",
    "    code_interpreter_tool = CodeInterpreterTool(file_ids=data_file_ids)\n",
    "else:\n",
    "    code_interpreter_tool = None\n",
    "    print(\"no data files found to upload\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine All Tools into a ToolSet\n",
    "This step creates a custom `ToolSet` that includes all the tools configured earlier.\n",
    "It also adds a `LoggingToolSet` subclass to log the inputs and outputs of function calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tool > FileSearchTool\n",
      "tool > CodeInterpreterTool\n",
      "tool > FunctionTool\n"
     ]
    }
   ],
   "source": [
    "class LoggingToolSet(ToolSet):\n",
    "    def execute_tool_calls(self, tool_calls: List[Any]) -> List[dict]:\n",
    "        \"\"\"\n",
    "        Execute the upstream calls, printing only two lines per function:\n",
    "        1) The function name + its input arguments\n",
    "        2) The function name + its output result\n",
    "        \"\"\"\n",
    "\n",
    "        # For each function call, print the input arguments\n",
    "        for c in tool_calls:\n",
    "            if hasattr(c, \"function\") and c.function:\n",
    "                fn_name = c.function.name\n",
    "                fn_args = c.function.arguments\n",
    "                print(f\"{fn_name} inputs > {fn_args} (id:{c.id})\")\n",
    "\n",
    "        # Execute the tool calls (superclass logic)\n",
    "        raw_outputs = super().execute_tool_calls(tool_calls)\n",
    "\n",
    "        # Print the output of each function call\n",
    "        for item in raw_outputs:\n",
    "            print(f\"output > {item['output']}\")\n",
    "\n",
    "        return raw_outputs\n",
    "\n",
    "custom_functions = FunctionTool(enterprise_fns)\n",
    "\n",
    "toolset = LoggingToolSet()\n",
    "# if bing_tool:\n",
    "#     toolset.add(bing_tool)\n",
    "if file_search_tool:\n",
    "    toolset.add(file_search_tool)\n",
    "if code_interpreter_tool:\n",
    "    toolset.add(code_interpreter_tool)\n",
    "toolset.add(custom_functions)\n",
    "\n",
    "for tool in toolset._tools:\n",
    "    tool_name = tool.__class__.__name__\n",
    "    print(f\"tool > {tool_name}\")\n",
    "    for definition in tool.definitions:\n",
    "        if hasattr(definition, \"function\"):\n",
    "            fn = definition.function\n",
    "            print(f\"{fn.name} > {fn.description}\")\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create or Reuse the Enterprise Agent\n",
    "In this step, we create a new enterprise agent or reuse an existing one.\n",
    "The agent is configured with a model, instructions, and the toolset from the previous step.\n",
    "\n",
    "Note:\n",
    "* You will need to delete the previous agent, while recreate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reusing agent > med-diagnostic-agent (id: asst_h0TzV6BRST4V9e3sc33jKYpW)\n"
     ]
    }
   ],
   "source": [
    "AGENT_NAME = \"med-diagnostic-agent\"\n",
    "found_agent = None\n",
    "all_agents_list = project_client.agents.list_agents()\n",
    "for a in all_agents_list:\n",
    "    if a.name == AGENT_NAME:\n",
    "        found_agent = a\n",
    "        break\n",
    "\n",
    "model_name = settings.model_deployment_name\n",
    "\n",
    "instructions = (\n",
    "    \"You are a helpful medical diagnostic assistant. \"\n",
    "    \"You have access to following tools. \\n\\n\"\n",
    "    \"## Tools:\\n\"\n",
    "    \" * file_search: get informaton about treatment best practices from your company knowledge\\n\"\n",
    "    \" * code_interpeter: get information for patient data analytics tasks relating to study data in CSV format. Always load all the CSV files as DataFrame, and join all DataFrame by PATIENT_ID, then execute the user query on joined DataFrame.\\n\"\n",
    "    \"\\n\"\n",
    "    \"## Instructions:\\n\"\n",
    "    \"You can use the all the tools to answer questions\\n\"\n",
    "    \"\\n\"\n",
    "    \"## Guidelines:\\n\"\n",
    "    \"Provide well-structured and professional answers. \"\n",
    "    \"This year is 2025\"\n",
    "    \"## Execute Plan:\\n\"\n",
    "    \"You will execute your plan step by step without further instruction, and return the final answer in the end.\\n\"\n",
    ")\n",
    "\n",
    "project_client.agents.enable_auto_function_calls(tools=toolset)\n",
    "if found_agent:\n",
    "    agent = project_client.agents.update_agent(\n",
    "        agent_id=found_agent.id,\n",
    "        model=model_name,\n",
    "        instructions=instructions,\n",
    "        toolset=toolset,\n",
    "    )\n",
    "    project_client.agents.enable_auto_function_calls(tools=toolset) \n",
    "    print(f\"reusing agent > {agent.name} (id: {agent.id})\")\n",
    "else:\n",
    "    agent = project_client.agents.create_agent(\n",
    "        model=model_name,\n",
    "        name=AGENT_NAME,\n",
    "        instructions=instructions,\n",
    "        toolset=toolset,\n",
    "    )\n",
    "    print(f\"creating agent > {agent.name} (id: {agent.id})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Conversation Thread\n",
    "In this step, we create a new conversation thread for the enterprise agent.\n",
    "Threads are used to manage and track conversations with the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread > created (id: thread_VEsCjDoSbi14s6DBFtdaJ2nM)\n"
     ]
    }
   ],
   "source": [
    "thread = project_client.agents.threads.create()\n",
    "print(f\"thread > created (id: {thread.id})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a Custom Event Handler\n",
    "Here, we define a custom event handler to manage logs and outputs for debugging.\n",
    "This handler will capture and display real-time events during the agent's operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyEventHandler(AgentEventHandler):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._current_message_id = None\n",
    "        self._accumulated_text = \"\"\n",
    "\n",
    "    def on_message_delta(self, delta: MessageDeltaChunk) -> None:\n",
    "        # If a new message id, start fresh\n",
    "        if delta.id != self._current_message_id:\n",
    "            # First, if we had an old message that wasn't completed, finish that line\n",
    "            if self._current_message_id is not None:\n",
    "                print()  # move to a new line\n",
    "            \n",
    "            self._current_message_id = delta.id\n",
    "            self._accumulated_text = \"\"\n",
    "            print(\"\\nassistant > \", end=\"\")  # prefix for new message\n",
    "\n",
    "        # Accumulate partial text\n",
    "        partial_text = \"\"\n",
    "        if delta.delta.content:\n",
    "            for chunk in delta.delta.content:\n",
    "                # partial_text += chunk.text.get(\"value\", \"\")\n",
    "                if isinstance(chunk, MessageDeltaTextContent):\n",
    "                    partial_text += chunk[\"text\"].get(\"value\", \"\")\n",
    "                elif isinstance(chunk, MessageDeltaImageFileContent):\n",
    "                    partial_text += chunk[\"image_file\"].get(\"file_id\", \"\")\n",
    "        self._accumulated_text += partial_text\n",
    "\n",
    "        # Print partial text with no newline\n",
    "        print(partial_text, end=\"\", flush=True)\n",
    "\n",
    "    def on_thread_message(self, message: ThreadMessage) -> None:\n",
    "        # When the assistant's entire message is \"completed\", print a final newline\n",
    "        if message.status == \"completed\" and message.role == \"assistant\":\n",
    "            print()  # done with this line\n",
    "            self._current_message_id = None\n",
    "            self._accumulated_text = \"\"\n",
    "        else:\n",
    "            # For other roles or statuses, you can log if you like:\n",
    "            print(f\"{message.status.name.lower()} (id: {message.id})\")\n",
    "\n",
    "    def on_thread_run(self, run: ThreadRun) -> None:\n",
    "        print(f\"status > {run.status.name.lower()}\")\n",
    "        if run.status == \"failed\":\n",
    "            print(f\"error > {run.last_error}\")\n",
    "\n",
    "    def on_run_step(self, step: RunStep) -> None:\n",
    "        print(f\"{step.type.name.lower()} > {step.status.name.lower()}\")\n",
    "\n",
    "    def on_run_step_delta(self, delta: RunStepDeltaChunk) -> None:\n",
    "        # If partial tool calls come in, we log them\n",
    "        if delta.delta.step_details and delta.delta.step_details.tool_calls:\n",
    "            for tcall in delta.delta.step_details.tool_calls:\n",
    "                if getattr(tcall, \"function\", None):\n",
    "                    if tcall.function.name is not None:\n",
    "                        print(f\"tool call > {tcall.function.name}\")\n",
    "\n",
    "    def on_unhandled_event(self, event_type: str, event_data):\n",
    "        print(f\"unhandled > {event_type} > {event_data}\")\n",
    "\n",
    "    def on_error(self, data: str) -> None:\n",
    "        print(f\"error > {data}\")\n",
    "\n",
    "    def on_done(self) -> None:\n",
    "        print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the Main Chat Functions\n",
    "These functions define how user messages and tool interactions are processed.\n",
    "It uses the agent's thread to handle conversations and streams partial responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_bing_query(request_url: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract the query string from something like:\n",
    "      https://api.bing.microsoft.com/v7.0/search?q=\"latest news about Microsoft January 2025\"\n",
    "    Returns: latest news about Microsoft January 2025\n",
    "    \"\"\"\n",
    "    match = re.search(r'q=\"([^\"]+)\"', request_url)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    # If no match, fall back to entire request_url\n",
    "    return request_url\n",
    "\n",
    "def extract_search_annotation(\n",
    "        annotation: MessageDeltaTextUrlCitationAnnotation, text_value_str: str) -> str:\n",
    "    \"\"\"\n",
    "    {'index': 0, 'type': 'text', 'text': {'value': '【3:1†Siemens fiscal report 2024】', 'annotations': [{'index': 0, 'type': 'url_citation', 'text': '【3:1†Siemens fiscal report 2024】', 'start_index': 1369, 'end_index': 1401, 'url_citation': {'url': 'doc_1', 'title': 'Siemens_Report_FY2024.pdf'}}]}}\n",
    "\n",
    "    {'index': 0, 'type': 'text', 'text': {'value': '【14:0†best_practices_lung_cancer.md】', 'annotations': [{'index': 0, 'type': 'file_citation', 'text': '【14:0†best_practices_lung_cancer.md】', 'start_index': 2486, 'end_index': 2522, 'file_citation': {'file_id': 'assistant-XfXFvez3N2CbpttpNb8MK4', 'quote': ''}}]}}\n",
    "    \"\"\"\n",
    "    if annotation[\"type\"] == \"url_citation\":\n",
    "        url = annotation[\"url_citation\"][\"url\"]\n",
    "        title = annotation[\"url_citation\"].get(\"title\", \"\")\n",
    "        start_idx = annotation.get(\"start_index\", 0)\n",
    "        end_idx = annotation.get(\"end_index\", 0)\n",
    "        return f\" [{text_value_str.strip()} {title}]({url}) ({start_idx}-{end_idx})\"\n",
    "    elif annotation[\"type\"] == \"file_citation\":\n",
    "        # file_id = annotation[\"file_citation\"][\"file_id\"]\n",
    "        file_id = \"\"\n",
    "        quote = annotation[\"file_citation\"].get(\"quote\", \"\")\n",
    "        title = annotation[\"text\"]\n",
    "        start_idx = annotation.get(\"start_index\", 0)\n",
    "        end_idx = annotation.get(\"end_index\", 0)\n",
    "        return f\" [{text_value_str.strip()} {title}]({file_id}) ({start_idx}-{end_idx})\"\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def convert_dict_to_chatmessage(msg: dict) -> ChatMessage:\n",
    "    \"\"\"\n",
    "    Convert a legacy dict-based message to a gr.ChatMessage.\n",
    "    Uses the 'metadata' sub-dict if present.\n",
    "    \"\"\"\n",
    "    return ChatMessage(\n",
    "        role=msg[\"role\"],\n",
    "        content=msg[\"content\"],\n",
    "        metadata=msg.get(\"metadata\", None)\n",
    "    )\n",
    "\n",
    "def azure_enterprise_chat(user_message: str, history: List[dict]):\n",
    "    \"\"\"\n",
    "    Accumulates partial function arguments into ChatMessage['content'], sets the\n",
    "    corresponding tool bubble status from \"pending\" to \"done\" on completion,\n",
    "    and also handles non-function calls like bing_grounding or file_search by appending a\n",
    "    \"pending\" bubble. Then it moves them to \"done\" once tool calls complete.\n",
    "\n",
    "    This function returns a list of ChatMessage objects directly (no dict conversion).\n",
    "    Your Gradio Chatbot should be type=\"messages\" to handle them properly.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert existing history from dict to ChatMessage\n",
    "    conversation = []\n",
    "    for msg_dict in history:\n",
    "        conversation.append(convert_dict_to_chatmessage(msg_dict))\n",
    "\n",
    "    # Append the user's new message\n",
    "    conversation.append(ChatMessage(role=\"user\", content=user_message))\n",
    "\n",
    "    # Immediately yield two outputs to clear the textbox\n",
    "    yield conversation, \"\"\n",
    "\n",
    "    # Post user message to the thread (for your back-end logic)\n",
    "    project_client.agents.messages.create(\n",
    "        thread_id=thread.id,\n",
    "        role=\"user\",\n",
    "        content=user_message\n",
    "    )\n",
    "\n",
    "    # Mappings for partial function calls\n",
    "    call_id_for_index: Dict[int, str] = {}\n",
    "    partial_calls_by_index: Dict[int, dict] = {}\n",
    "    partial_calls_by_id: Dict[str, dict] = {}\n",
    "    in_progress_tools: Dict[str, ChatMessage] = {}\n",
    "\n",
    "    # Titles for tool bubbles\n",
    "    function_titles = {\n",
    "        # \"fetch_weather\": \"☁️ fetching weather\",\n",
    "        \"fetch_datetime\": \"🕒 fetching datetime\",\n",
    "        # \"fetch_stock_price\": \"📈 fetching financial info\",\n",
    "        # \"send_email\": \"✉️ sending mail\",\n",
    "        \"file_search\": \"📄 searching docs\",\n",
    "        \"bing_grounding\": \"🔍 searching bing\",\n",
    "        \"code_interpreter\": \"📊 analyzing data\",\n",
    "    }\n",
    "\n",
    "    def get_function_title(fn_name: str) -> str:\n",
    "        return function_titles.get(fn_name, f\"🛠 calling {fn_name}\")\n",
    "\n",
    "    def accumulate_args(storage: dict, name_chunk: str, arg_chunk: str):\n",
    "        \"\"\"Accumulates partial JSON data for a function call.\"\"\"\n",
    "        if name_chunk:\n",
    "            storage[\"name\"] += name_chunk\n",
    "        if arg_chunk:\n",
    "            storage[\"args\"] += arg_chunk\n",
    "\n",
    "    def finalize_tool_call(call_id: str):\n",
    "        \"\"\"Creates or updates the ChatMessage bubble for a function call.\"\"\"\n",
    "        if call_id not in partial_calls_by_id:\n",
    "            return\n",
    "        data = partial_calls_by_id[call_id]\n",
    "        fn_name = data[\"name\"].strip()\n",
    "        fn_args = data[\"args\"].strip()\n",
    "        if not fn_name:\n",
    "            return\n",
    "\n",
    "        if call_id not in in_progress_tools:\n",
    "            # Create a new bubble with status=\"pending\"\n",
    "            msg_obj = ChatMessage(\n",
    "                role=\"assistant\",\n",
    "                content=fn_args or \"\",\n",
    "                metadata={\n",
    "                    \"title\": get_function_title(fn_name),\n",
    "                    \"status\": \"pending\",\n",
    "                    \"id\": f\"tool-{call_id}\"\n",
    "                }\n",
    "            )\n",
    "            conversation.append(msg_obj)\n",
    "            in_progress_tools[call_id] = msg_obj\n",
    "        else:\n",
    "            # Update existing bubble\n",
    "            msg_obj = in_progress_tools[call_id]\n",
    "            msg_obj.content = fn_args or \"\"\n",
    "            msg_obj.metadata[\"title\"] = get_function_title(fn_name)\n",
    "\n",
    "    def upsert_tool_call(tcall: dict):\n",
    "        \"\"\"\n",
    "        1) Check the call type\n",
    "        2) If \"function\", gather partial name/args\n",
    "        3) If \"bing_grounding\" or \"file_search\", show a pending bubble\n",
    "        4) If \"code_interpreter\", create only one bubble\n",
    "        \"\"\"\n",
    "        t_type = tcall.get(\"type\", \"\")\n",
    "        call_id = tcall.get(\"id\", None)\n",
    "\n",
    "        # If call_id is None, generate a unique one (for parallel calls)\n",
    "        if call_id is None:\n",
    "            if t_type in (\"file_search\", \"bing_grounding\"):\n",
    "                call_id = str(uuid.uuid4())\n",
    "            elif t_type == \"code_interpreter\":\n",
    "                call_id = \"code_interpreter\"\n",
    "            elif t_type == \"azure_ai_search\":\n",
    "                call_id = \"azure_ai_search\"\n",
    "            else:\n",
    "                call_id = \"unknown_tool\"\n",
    "\n",
    "        # --- BING GROUNDING ---\n",
    "        if t_type == \"bing_grounding\":\n",
    "            request_url = tcall.get(\"bing_grounding\", {}).get(\"requesturl\", \"\")\n",
    "            if not request_url.strip():\n",
    "                return\n",
    "\n",
    "            query_str = extract_bing_query(request_url)\n",
    "            if not query_str.strip():\n",
    "                return\n",
    "\n",
    "            msg_obj = ChatMessage(\n",
    "                role=\"assistant\",\n",
    "                content=query_str,\n",
    "                metadata={\n",
    "                    \"title\": get_function_title(\"bing_grounding\"),\n",
    "                    \"status\": \"pending\",\n",
    "                    \"id\": f\"tool-{call_id}\" if call_id else \"tool-noid\"\n",
    "                }\n",
    "            )\n",
    "            conversation.append(msg_obj)\n",
    "            if call_id is not None:\n",
    "                in_progress_tools[call_id] = msg_obj\n",
    "            return\n",
    "\n",
    "        # --- FILE SEARCH ---\n",
    "        elif t_type == \"file_search\":\n",
    "            msg_obj = ChatMessage(\n",
    "                role=\"assistant\",\n",
    "                content=\"searching docs...\",\n",
    "                metadata={\n",
    "                    \"title\": get_function_title(\"file_search\"),\n",
    "                    \"status\": \"pending\",\n",
    "                    \"id\": f\"tool-{call_id}\" if call_id else \"tool-noid\"\n",
    "                }\n",
    "            )\n",
    "            conversation.append(msg_obj)\n",
    "            if call_id is not None:\n",
    "                in_progress_tools[call_id] = msg_obj\n",
    "            return\n",
    "        \n",
    "        # --- Azure AI SEARCH --- \n",
    "        elif t_type == \"azure_ai_search\":\n",
    "            if call_id not in in_progress_tools:\n",
    "                msg_obj = ChatMessage(\n",
    "                    role=\"assistant\",\n",
    "                    content=\"searching private index...\",\n",
    "                    metadata={\n",
    "                        \"title\": get_function_title(\"azure_ai_search\"),\n",
    "                        \"status\": \"pending\",\n",
    "                        \"id\": f\"tool-{call_id}\" if call_id else \"tool-noid\"\n",
    "                    }\n",
    "                )\n",
    "                conversation.append(msg_obj)\n",
    "                in_progress_tools[call_id] = msg_obj\n",
    "            return\n",
    "        \n",
    "        # -- CODE INTERPRETER ---\n",
    "        elif t_type == \"code_interpreter\":\n",
    "            if call_id not in in_progress_tools:\n",
    "                msg_obj = ChatMessage(\n",
    "                    role=\"assistant\",\n",
    "                    content=\"analyzing data...\",\n",
    "                    metadata={\n",
    "                        \"title\": get_function_title(\"code_interpreter\"),\n",
    "                        \"status\": \"pending\",\n",
    "                        \"id\": f\"tool-{call_id}\"\n",
    "                    }\n",
    "                )\n",
    "                conversation.append(msg_obj)\n",
    "                in_progress_tools[call_id] = msg_obj\n",
    "            return\n",
    "\n",
    "        # --- NON-FUNCTION CALLS ---\n",
    "        elif t_type != \"function\":\n",
    "            return\n",
    "\n",
    "        # --- FUNCTION CALL PARTIAL-ARGS ---\n",
    "        index = tcall.get(\"index\")\n",
    "        new_call_id = call_id\n",
    "        fn_data = tcall.get(\"function\", {})\n",
    "        name_chunk = fn_data.get(\"name\", \"\")\n",
    "        arg_chunk = fn_data.get(\"arguments\", \"\")\n",
    "\n",
    "        if new_call_id:\n",
    "            call_id_for_index[index] = new_call_id\n",
    "\n",
    "        call_id = call_id_for_index.get(index)\n",
    "        if not call_id:\n",
    "            # Accumulate partial\n",
    "            if index not in partial_calls_by_index:\n",
    "                partial_calls_by_index[index] = {\"name\": \"\", \"args\": \"\"}\n",
    "            accumulate_args(partial_calls_by_index[index], name_chunk, arg_chunk)\n",
    "            return\n",
    "\n",
    "        if call_id not in partial_calls_by_id:\n",
    "            partial_calls_by_id[call_id] = {\"name\": \"\", \"args\": \"\"}\n",
    "\n",
    "        if index in partial_calls_by_index:\n",
    "            old_data = partial_calls_by_index.pop(index)\n",
    "            partial_calls_by_id[call_id][\"name\"] += old_data.get(\"name\", \"\")\n",
    "            partial_calls_by_id[call_id][\"args\"] += old_data.get(\"args\", \"\")\n",
    "\n",
    "        # Accumulate partial\n",
    "        accumulate_args(partial_calls_by_id[call_id], name_chunk, arg_chunk)\n",
    "\n",
    "        # Create/update the function bubble\n",
    "        finalize_tool_call(call_id)\n",
    "\n",
    "    # -- EVENT STREAMING --\n",
    "    with project_client.agents.runs.stream(\n",
    "        thread_id=thread.id,\n",
    "        agent_id=agent.id,\n",
    "        # assistant_id=agent.id,\n",
    "        event_handler=MyEventHandler(),  # the event handler handles console output\n",
    "        # parallel_tool_calls=True,\n",
    "        tool_choice=AgentsNamedToolChoice(\n",
    "            type=AgentsNamedToolChoiceType.CODE_INTERPRETER), \n",
    "    ) as stream:\n",
    "        # pulling the result from the stream manually\n",
    "        for item in stream:\n",
    "            event_type, event_data, *_ = item\n",
    "\n",
    "            # Remove any None items that might have been appended\n",
    "            conversation = [m for m in conversation if m is not None]\n",
    "\n",
    "            # 1) Partial tool calls\n",
    "            if event_type == \"thread.run.step.delta\":\n",
    "                step_delta = event_data.get(\"delta\", {}).get(\"step_details\", {})\n",
    "                if step_delta.get(\"type\") == \"tool_calls\":\n",
    "                    for tcall in step_delta.get(\"tool_calls\", []):\n",
    "                        upsert_tool_call(tcall)\n",
    "                    yield conversation, \"\"\n",
    "\n",
    "            # 2) run_step\n",
    "            elif event_type == \"run_step\":\n",
    "                step_type = event_data[\"type\"]\n",
    "                step_status = event_data[\"status\"]\n",
    "\n",
    "                # If tool calls are in progress, new or partial\n",
    "                if step_type == \"tool_calls\" and step_status == \"in_progress\":\n",
    "                    for tcall in event_data[\"step_details\"].get(\"tool_calls\", []):\n",
    "                        upsert_tool_call(tcall)\n",
    "                    yield conversation, \"\"\n",
    "\n",
    "                elif step_type == \"tool_calls\" and step_status == \"completed\":\n",
    "                    for cid, msg_obj in in_progress_tools.items():\n",
    "                        msg_obj.metadata[\"status\"] = \"done\"\n",
    "                    in_progress_tools.clear()\n",
    "                    partial_calls_by_id.clear()\n",
    "                    partial_calls_by_index.clear()\n",
    "                    call_id_for_index.clear()\n",
    "                    yield conversation, \"\"\n",
    "\n",
    "                elif step_type == \"message_creation\" and step_status == \"in_progress\":\n",
    "                    msg_id = event_data[\"step_details\"][\"message_creation\"].get(\"message_id\")\n",
    "                    if msg_id:\n",
    "                        conversation.append(ChatMessage(role=\"assistant\", content=\"\"))\n",
    "                    yield conversation, \"\"\n",
    "\n",
    "                elif step_type == \"message_creation\" and step_status == \"completed\":\n",
    "                    yield conversation, \"\"\n",
    "\n",
    "            # 3) partial text from the assistant\n",
    "            elif event_type == \"thread.message.delta\":\n",
    "                agent_msg = \"\"\n",
    "                for chunk in event_data[\"delta\"][\"content\"]:\n",
    "                    # print(\"chunk > \", chunk)\n",
    "                    # print(\"chunk type > \", type(chunk))\n",
    "                    if isinstance(chunk, MessageDeltaTextContent):\n",
    "                        # Safely get the text value\n",
    "                        text_obj: MessageDeltaTextContentObject = chunk.get(\"text\", {})\n",
    "                        text_value_str = text_obj.get(\"value\", \"\")\n",
    "                        annotations = text_obj.get(\"annotations\", None)\n",
    "                        if annotations:\n",
    "                            # Extract the URL citation if available\n",
    "                            for annotation in annotations: \n",
    "                                agent_msg += extract_search_annotation(annotation, text_value_str)\n",
    "                        else:\n",
    "                            agent_msg += text_value_str\n",
    "                            \n",
    "                    elif isinstance(chunk, MessageDeltaImageFileContent):\n",
    "                        file_id = chunk[\"image_file\"].get(\"file_id\", \"\")\n",
    "                        byte_stream = project_client.agents.files.get_content(file_id=file_id)\n",
    "                        # Encode to base64\n",
    "                        # Join all bytes from the iterator\n",
    "                        image_bytes = b\"\".join(byte_stream)\n",
    "                        b64_image = base64.b64encode(image_bytes).decode(\"utf-8\")\n",
    "                        # Use Markdown to display the image in Gradio\n",
    "                        agent_msg += f\"![image](data:image/png;base64,{b64_image})\"  \n",
    "\n",
    "                message_id = event_data[\"id\"]\n",
    "\n",
    "                # Try to find a matching assistant bubble\n",
    "                matching_msg = None\n",
    "                for msg in reversed(conversation):\n",
    "                    if msg.metadata and msg.metadata.get(\"id\") == message_id and msg.role == \"assistant\":\n",
    "                        matching_msg = msg\n",
    "                        break\n",
    "\n",
    "                if matching_msg:\n",
    "                    # Append newly streamed text\n",
    "                    matching_msg.content += agent_msg\n",
    "                else:\n",
    "                    # Append to last assistant or create new\n",
    "                    if (\n",
    "                        not conversation\n",
    "                        or conversation[-1].role != \"assistant\"\n",
    "                        or (\n",
    "                            conversation[-1].metadata\n",
    "                            and str(conversation[-1].metadata.get(\"id\", \"\")).startswith(\"tool-\")\n",
    "                        )\n",
    "                    ):\n",
    "                        conversation.append(ChatMessage(role=\"assistant\", content=agent_msg))\n",
    "                    else:\n",
    "                        conversation[-1].content += agent_msg\n",
    "\n",
    "                yield conversation, \"\"\n",
    "\n",
    "            # 4) If entire assistant message is completed\n",
    "            elif event_type == \"thread.message\":\n",
    "                if event_data[\"role\"] == \"assistant\" and event_data[\"status\"] == \"completed\":\n",
    "                    for cid, msg_obj in in_progress_tools.items():\n",
    "                        msg_obj.metadata[\"status\"] = \"done\"\n",
    "                    in_progress_tools.clear()\n",
    "                    partial_calls_by_id.clear()\n",
    "                    partial_calls_by_index.clear()\n",
    "                    call_id_for_index.clear()\n",
    "                    yield conversation, \"\"\n",
    "\n",
    "            # 5) Final done\n",
    "            elif event_type == \"thread.message.completed\":\n",
    "                for cid, msg_obj in in_progress_tools.items():\n",
    "                    msg_obj.metadata[\"status\"] = \"done\"\n",
    "                in_progress_tools.clear()\n",
    "                partial_calls_by_id.clear()\n",
    "                partial_calls_by_index.clear()\n",
    "                call_id_for_index.clear()\n",
    "                yield conversation, \"\"\n",
    "                break\n",
    "\n",
    "    return conversation, \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Gradio UI\n",
    "Create a Gradio interface for interacting with the enterprise agent. \n",
    "Include a chatbot component and a text input box for user queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note:\n",
      "tool calling may fail, close the chat with trash bin icon (🗑️) and rerun the prompt.\n",
      "\n",
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "brand_theme = gr.themes.Default(\n",
    "    primary_hue=\"blue\",\n",
    "    secondary_hue=\"blue\",\n",
    "    neutral_hue=\"gray\",\n",
    "    font=[\"Segoe UI\", \"Arial\", \"sans-serif\"],\n",
    "    font_mono=[\"Courier New\", \"monospace\"],\n",
    "    text_size=\"lg\",\n",
    ").set(\n",
    "    button_primary_background_fill=\"#0f6cbd\",\n",
    "    button_primary_background_fill_hover=\"#115ea3\",\n",
    "    button_primary_background_fill_hover_dark=\"#4f52b2\",\n",
    "    button_primary_background_fill_dark=\"#5b5fc7\",\n",
    "    button_primary_text_color=\"#ffffff\",\n",
    "    button_secondary_background_fill=\"#e0e0e0\",\n",
    "    button_secondary_background_fill_hover=\"#c0c0c0\",\n",
    "    button_secondary_background_fill_hover_dark=\"#a0a0a0\",\n",
    "    button_secondary_text_color=\"#000000\",\n",
    "    body_background_fill=\"#f5f5f5\",\n",
    "    block_background_fill=\"#ffffff\",\n",
    "    body_text_color=\"#242424\",\n",
    "    body_text_color_subdued=\"#616161\",\n",
    "    block_border_color=\"#d1d1d1\",\n",
    "    block_border_color_dark=\"#333333\",\n",
    "    input_background_fill=\"#ffffff\",\n",
    "    input_border_color=\"#d1d1d1\",\n",
    "    input_border_color_focus=\"#0f6cbd\",\n",
    ")\n",
    "\n",
    "with gr.Blocks(theme=brand_theme, css=\"footer {visibility: hidden;}\", fill_height=True) as demo:\n",
    "\n",
    "    def clear_thread():\n",
    "        global thread\n",
    "        thread = project_client.agents.threads.create()\n",
    "        return []\n",
    "\n",
    "    def on_example_clicked(evt: gr.SelectData):\n",
    "        return evt.value[\"text\"]  # Fill the textbox with that example text\n",
    "\n",
    "    gr.HTML(\"<h1 style=\\\"text-align: center;\\\">Azure AI Agent Service</h1>\")\n",
    "    \n",
    "    # patient 10\n",
    "    # Cough and wheezing for 5 months.\n",
    "    # You need to \"continue\" if the Agent generate a plan.\n",
    "    chatbot = gr.Chatbot(\n",
    "        type=\"messages\",\n",
    "        examples=[\n",
    "            # {\"text\": \"Patient 10 has\\n1.) The aorta is atherosclerotic.\\n2.) A 4.5 cm mass is in the left lower lobe.\\n3.) There are chronic healed bilateral rib fractures.\\n4.) The lungs are hyperinflated.The heart size is normal.\"},\n",
    "            {\"text\": \"Is patient with PATIENT_ID 10 an outlier in the lung cancer study dataset focusing on symptoms and risk factors, Use LOF and visulize the result in a box plot for all normalized features?\"},\n",
    "            {\"text\": \"Suggest a treatment plan with research innovation from best practices for patient with PATIENT_ID 10.\"},\n",
    "        ],\n",
    "        show_label=False,\n",
    "        scale=1,\n",
    "    )\n",
    "\n",
    "    textbox = gr.Textbox(\n",
    "        show_label=False,\n",
    "        lines=1,\n",
    "        submit_btn=True,\n",
    "    )\n",
    "\n",
    "    # Populate textbox when an example is clicked\n",
    "    chatbot.example_select(fn=on_example_clicked, inputs=None, outputs=textbox)\n",
    "\n",
    "    # On submit: call azure_enterprise_chat, then clear the textbox\n",
    "    (textbox\n",
    "     .submit(\n",
    "         fn=azure_enterprise_chat,\n",
    "         inputs=[textbox, chatbot],\n",
    "         outputs=[chatbot, textbox],\n",
    "     )\n",
    "     .then(\n",
    "         fn=lambda: \"\",\n",
    "         outputs=textbox,\n",
    "     )\n",
    "    )\n",
    "\n",
    "    # A \"Clear\" button that resets the thread and the Chatbot\n",
    "    chatbot.clear(fn=clear_thread, outputs=chatbot)\n",
    "\n",
    "# Launch your Gradio app\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Note:\\ntool calling may fail, close the chat with trash bin icon (🗑️) and rerun the prompt.\\n\")\n",
    "    demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) delete agent, thread, and vector store resources\n",
    "Uncomment out the next cell block to delete the resources created in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from azure.identity import DefaultAzureCredential\n",
    "# from azure.ai.projects import AIProjectClient\n",
    "# import os\n",
    "\n",
    "# credential = DefaultAzureCredential()\n",
    "# project_client_delete = AIProjectClient(\n",
    "#    credential=credential,\n",
    "#    endpoint=os.environ.get(\"PROJECT_ENDPOINT\")\n",
    "# )\n",
    "\n",
    "# try:\n",
    "#    project_client_delete.agents.delete_agent(agent.id)\n",
    "#    print(\"Agent deletion successful.\")\n",
    "#    project_client_delete.agents.threads.delete(thread.id)\n",
    "#    print(\"Thread deletion successful.\")\n",
    "#    project_client_delete.agents.vector_stores.delete(vector_store_id)\n",
    "#    print(\"Vector store deletion successful.\")\n",
    "#    print(\"All deletions succeeded.\")\n",
    "# except Exception as e:\n",
    "#    print(f\"Error during deletion: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "status > queued\n",
      "status > queued\n",
      "status > in_progress\n",
      "tool_calls > in_progress\n",
      "tool_calls > in_progress\n",
      "tool_calls > completed\n",
      "message_creation > in_progress\n",
      "message_creation > in_progress\n",
      "in_progress (id: msg_NvixS4N9XNh0DpoTjb8LL4ew)\n",
      "in_progress (id: msg_NvixS4N9XNh0DpoTjb8LL4ew)\n",
      "\n",
      "assistant > There are two datasets uploaded about the lung cancer study, both having PATIENT_ID and some overlapping variables about symptoms (coughing, shortness_of_breath, chest_pain), but the first dataset has a more comprehensive set of variables including risk factors and demographic info.\n",
      "\n",
      "I will join these datasets by PATIENT_ID to have a complete dataset. Then I will:\n",
      "\n",
      "1. Focus on symptoms and risk factors columns.\n",
      "2. Normalize the features.\n",
      "3. Use Local Outlier Factor (LOF) method to detect if patient with PATIENT_ID 10 is an outlier.\n",
      "4. Visualize the LOF results including patient 10 in a box plot for all normalized features.\n",
      "\n",
      "Let's start by joining the datasets and preparing the data.\n",
      "status > queued\n",
      "status > queued\n",
      "status > in_progress\n",
      "tool_calls > in_progress\n",
      "tool_calls > in_progress\n",
      "tool_calls > completed\n",
      "message_creation > in_progress\n",
      "message_creation > in_progress\n",
      "in_progress (id: msg_HiWUCV0tx4Stw8pqry7MCHsp)\n",
      "in_progress (id: msg_HiWUCV0tx4Stw8pqry7MCHsp)\n",
      "\n",
      "assistant > assistant-CDbY8BQmNmo2ZErmLkUwyHThe analysis shows that the patient with PATIENT_ID 10 is not present in the lung cancer study dataset provided.\n",
      "\n",
      "Using Local Outlier Factor (LOF) on all normalized symptom and risk factor features in the dataset, no patients were detected as outliers (0 outliers). I have also visualized the box plot of all normalized features across patients for your reference.\n",
      "\n",
      "If you would like to check outlier status for another patient or want me to analyze outliers differently, please let me know.\n"
     ]
    }
   ],
   "source": [
    "# existing_stores = project_client_delete.agents.vector_stores.list()\n",
    "# for store in existing_stores:\n",
    "#     project_client_delete.agents.vector_stores.delete(store.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azfdydemo3.12pip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
