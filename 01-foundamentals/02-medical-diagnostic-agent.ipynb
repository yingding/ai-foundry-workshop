{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure AI Agent Service Diagnostic Demo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Necessary Libraries\n",
    "In this cell, we import all the libraries and modules required for the project.\n",
    "This includes Azure AI SDKs, Gradio for UI, and custom functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment and authentication OK\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import uuid\n",
    "from datetime import datetime as pydatetime\n",
    "from typing import Any, List, Dict\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# (Optional) Gradio app for UI\n",
    "import gradio as gr\n",
    "from gradio import ChatMessage\n",
    "import base64\n",
    "\n",
    "# Azure AI Projects\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "from azure.ai.projects import AIProjectClient\n",
    "import azure.ai.agents as agentslib\n",
    "import azure.ai.projects as projectslib\n",
    "from azure.ai.agents.models import (\n",
    "    AgentEventHandler,\n",
    "    RunStep,\n",
    "    RunStepDeltaChunk,\n",
    "    ThreadMessage,\n",
    "    ThreadRun,\n",
    "    MessageDeltaChunk,\n",
    "    BingGroundingTool,\n",
    "    FilePurpose,\n",
    "    FileSearchTool,\n",
    "    FunctionTool,\n",
    "    ToolSet,\n",
    "    VectorStore,\n",
    "    AzureAISearchTool,\n",
    "    CodeInterpreterTool,\n",
    "    MessageDeltaTextContent,\n",
    "    MessageDeltaImageFileContent,\n",
    "    MessageDeltaTextContentObject,\n",
    "    MessageDeltaTextUrlCitationAnnotation,\n",
    "    MessageRole,\n",
    "    AgentThread,\n",
    "    MessageTextContent,\n",
    "    AgentsNamedToolChoice,\n",
    "    AgentsToolChoiceOptionMode,\n",
    "    AgentsNamedToolChoiceType,\n",
    ")\n",
    "\n",
    "# Your custom Python functions (for \"fetch_datetime\", etc.)\n",
    "from utils.enterprise_functions import enterprise_fns\n",
    "\n",
    "load_dotenv(dotenv_path=\".env\", override=True)\n",
    "\n",
    "from utils.fdyauth import AuthHelper\n",
    "settings = AuthHelper.load_settings()\n",
    "credential = AuthHelper.test_credential()\n",
    "\n",
    "if credential:\n",
    "    print('Environment and authentication OK')\n",
    "else:\n",
    "    print(\"please login first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Client and Load Azure AI Foundry\n",
    "Here, we initialize the Azure AI client using DefaultAzureCredential.\n",
    "This allows us to authenticate and connect to the Azure AI service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project_client api version: 2025-05-15-preview\n",
      "azure-ai-agents version: 1.1.0b3\n",
      "azure-ai-projects version: 1.0.0b12\n"
     ]
    }
   ],
   "source": [
    "# new AI Foundry Project resource endpoint / old azure ai services endpoint from the hub/project\n",
    "project_client = AIProjectClient(\n",
    "    credential=credential,\n",
    "    endpoint=settings.project_endpoint,\n",
    "    # api_version=os.environ[\"PROJECT_API_VERSION\"]\n",
    ")\n",
    "print(\"project_client api version:\", project_client._config.api_version)\n",
    "print(f\"azure-ai-agents version: {agentslib.__version__}\")\n",
    "print(f\"azure-ai-projects version: {projectslib.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up Tools (BingGroundingTool, FileSearchTool)\n",
    "In this step, we configure tools such as `BingGroundingTool` and `FileSearchTool`.\n",
    "We check for existing connections and create or reuse vector stores for document search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "If you see the following cell has error:\n",
    "```\n",
    "AzureCliCredential: Please run 'az login' to set up an account\n",
    "```\n",
    "\n",
    "relogin from powershell\n",
    "```powershell\n",
    "az logout\n",
    "az account clear\n",
    "az login --tenant 00000000-0000-0000-0000-000000000000\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reusing vector store > lung-treatment-vector-store (id: vs_kvDdrd5AJnCrmuRFP4fJkJWp)\n"
     ]
    }
   ],
   "source": [
    "# try:\n",
    "#     bing_connection = project_client.connections.get(name=os.environ[\"BING_CONNECTION_NAME\"])\n",
    "#     # print(f\"{bing_connection}\")\n",
    "#     conn_id = bing_connection.id\n",
    "#     bing_tool = BingGroundingTool(connection_id=conn_id)\n",
    "#     print(\"bing > connected\")\n",
    "# except Exception:\n",
    "#     bing_tool = None\n",
    "#     print(\"bing failed > no connection found or permission issue\")\n",
    "\n",
    "## need to be wrapped inside the agents_client, close agents_client if done\n",
    "FOLDER_NAME = \"treatment-data\"\n",
    "VECTOR_STORE_NAME = \"lung-treatment-vector-store\"\n",
    "\n",
    "# project_client.agents return the AgentsClient\n",
    "all_vector_stores: List[VectorStore] = project_client.agents.vector_stores.list()\n",
    "\n",
    "existing_vector_store = next(\n",
    "    (store for store in all_vector_stores if store.name == VECTOR_STORE_NAME),\n",
    "    None\n",
    ")\n",
    "\n",
    "vector_store_id = None\n",
    "if existing_vector_store:\n",
    "    vector_store_id = existing_vector_store.id\n",
    "    print(f\"reusing vector store > {existing_vector_store.name} (id: {existing_vector_store.id})\")\n",
    "else:\n",
    "    # If you have local docs to upload\n",
    "    import os\n",
    "    if os.path.isdir(FOLDER_NAME):\n",
    "        file_ids = []\n",
    "        for file_name in os.listdir(FOLDER_NAME):\n",
    "            file_path = os.path.join(FOLDER_NAME, file_name)\n",
    "            if os.path.isfile(file_path):\n",
    "                print(f\"uploading > {file_name}\")\n",
    "                uploaded_file = project_client.agents.files.upload_and_poll(\n",
    "                    file_path=file_path,\n",
    "                    purpose=FilePurpose.AGENTS\n",
    "                )\n",
    "                file_ids.append(uploaded_file.id)\n",
    "\n",
    "        if file_ids:\n",
    "            print(f\"creating vector store > from {len(file_ids)} files.\")\n",
    "            vector_store = project_client.agents.vector_stores.create_and_poll(\n",
    "                file_ids=file_ids,\n",
    "                name=VECTOR_STORE_NAME\n",
    "            )\n",
    "            vector_store_id = vector_store.id\n",
    "            print(f\"created > {vector_store.name} (id: {vector_store_id})\")\n",
    "\n",
    "file_search_tool = None\n",
    "if vector_store_id:\n",
    "    file_search_tool = FileSearchTool(vector_store_ids=[vector_store_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up Tools (Coding Interpreter)\n",
    "In this step, we configure tools such as `CodeInterpreterTool`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file already uploaded > lung_cancer_patients_symptoms_2025.csv (id: assistant-GJ6hZx35ajvkaqjsjzrbTB)\n",
      "file already uploaded > lung_cancer_patient_clinical_features.csv (id: assistant-5ri9uw9EoDCqkYrv5iZvep)\n"
     ]
    }
   ],
   "source": [
    "# Create Code Interpreter\n",
    "DATA_FOLDER_NAME = \"medicare-data\"\n",
    "uploaded_files_list = project_client.agents.files.list(purpose=FilePurpose.AGENTS).data\n",
    "\n",
    "# dictionary of uploaded files list with key file.filename, value file.id\n",
    "uploaded_files_dict = {file.filename: file.id for file in uploaded_files_list}\n",
    "\n",
    "data_file_ids = []\n",
    "if os.path.isdir(DATA_FOLDER_NAME):    \n",
    "    for file_name in os.listdir(DATA_FOLDER_NAME):\n",
    "        file_path = os.path.join(DATA_FOLDER_NAME, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            # test file_name exists as key in dictionary uploaded_files_dict\n",
    "            if file_name in uploaded_files_dict:\n",
    "                data_file_id = uploaded_files_dict[file_name]\n",
    "                data_file_ids.append(data_file_id)\n",
    "                print(f\"file already uploaded > {file_name} (id: {data_file_id})\")\n",
    "            else:\n",
    "                print(f\"uploading > {file_name}\")\n",
    "                uploaded_file = project_client.agents.files.upload_and_poll(\n",
    "                    file_path=file_path,\n",
    "                    purpose=FilePurpose.AGENTS\n",
    "                )\n",
    "                data_file_ids.append(uploaded_file.id)\n",
    "\n",
    "if len(data_file_ids) > 0:\n",
    "    code_interpreter_tool = CodeInterpreterTool(file_ids=data_file_ids)\n",
    "else:\n",
    "    code_interpreter_tool = None\n",
    "    print(\"no data files found to upload\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine All Tools into a ToolSet\n",
    "This step creates a custom `ToolSet` that includes all the tools configured earlier.\n",
    "It also adds a `LoggingToolSet` subclass to log the inputs and outputs of function calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tool > FileSearchTool\n",
      "tool > CodeInterpreterTool\n",
      "tool > FunctionTool\n"
     ]
    }
   ],
   "source": [
    "class LoggingToolSet(ToolSet):\n",
    "    def execute_tool_calls(self, tool_calls: List[Any]) -> List[dict]:\n",
    "        \"\"\"\n",
    "        Execute the upstream calls, printing only two lines per function:\n",
    "        1) The function name + its input arguments\n",
    "        2) The function name + its output result\n",
    "        \"\"\"\n",
    "\n",
    "        # For each function call, print the input arguments\n",
    "        for c in tool_calls:\n",
    "            if hasattr(c, \"function\") and c.function:\n",
    "                fn_name = c.function.name\n",
    "                fn_args = c.function.arguments\n",
    "                print(f\"{fn_name} inputs > {fn_args} (id:{c.id})\")\n",
    "\n",
    "        # Execute the tool calls (superclass logic)\n",
    "        raw_outputs = super().execute_tool_calls(tool_calls)\n",
    "\n",
    "        # Print the output of each function call\n",
    "        for item in raw_outputs:\n",
    "            print(f\"output > {item['output']}\")\n",
    "\n",
    "        return raw_outputs\n",
    "\n",
    "custom_functions = FunctionTool(enterprise_fns)\n",
    "\n",
    "toolset = LoggingToolSet()\n",
    "# if bing_tool:\n",
    "#     toolset.add(bing_tool)\n",
    "if file_search_tool:\n",
    "    toolset.add(file_search_tool)\n",
    "if code_interpreter_tool:\n",
    "    toolset.add(code_interpreter_tool)\n",
    "toolset.add(custom_functions)\n",
    "\n",
    "for tool in toolset._tools:\n",
    "    tool_name = tool.__class__.__name__\n",
    "    print(f\"tool > {tool_name}\")\n",
    "    for definition in tool.definitions:\n",
    "        if hasattr(definition, \"function\"):\n",
    "            fn = definition.function\n",
    "            print(f\"{fn.name} > {fn.description}\")\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create or Reuse the Enterprise Agent\n",
    "In this step, we create a new enterprise agent or reuse an existing one.\n",
    "The agent is configured with a model, instructions, and the toolset from the previous step.\n",
    "\n",
    "Note:\n",
    "* You will need to delete the previous agent, while recreate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reusing agent > med-diagnostic-agent (id: asst_h0TzV6BRST4V9e3sc33jKYpW)\n"
     ]
    }
   ],
   "source": [
    "AGENT_NAME = \"med-diagnostic-agent\"\n",
    "found_agent = None\n",
    "all_agents_list = project_client.agents.list_agents()\n",
    "for a in all_agents_list:\n",
    "    if a.name == AGENT_NAME:\n",
    "        found_agent = a\n",
    "        break\n",
    "\n",
    "model_name = settings.model_deployment_name\n",
    "\n",
    "instructions = (\n",
    "    \"You are a helpful medical diagnostic assistant. \"\n",
    "    \"You have access to following tools. \\n\\n\"\n",
    "    \"## Tools:\\n\"\n",
    "    \" * file_search: get informaton about treatment best practices from your company knowledge\\n\"\n",
    "    \" * code_interpeter: get information for patient data analytics tasks relating to study data in CSV format. Always load all the CSV files with no index column as DataFrame, and then decide how dataset can help answer the questions.\\n\"\n",
    "    \"\\n\"\n",
    "    \"## Instructions:\\n\"\n",
    "    \"You can use the all the tools to answer questions\\n\"\n",
    "    \"\\n\"\n",
    "    \"## Guidelines:\\n\"\n",
    "    \"Provide well-structured and professional answers. \"\n",
    "    \"This year is 2025\"\n",
    "    # \"## Execute Plan:\\n\"\n",
    "    # \"You will execute your plan step by step without further instruction, and return the final answer in the end.\\n\"\n",
    ")\n",
    "\n",
    "project_client.agents.enable_auto_function_calls(tools=toolset)\n",
    "if found_agent:\n",
    "    agent = project_client.agents.update_agent(\n",
    "        agent_id=found_agent.id,\n",
    "        model=model_name,\n",
    "        instructions=instructions,\n",
    "        toolset=toolset,\n",
    "    )\n",
    "    project_client.agents.enable_auto_function_calls(tools=toolset) \n",
    "    print(f\"reusing agent > {agent.name} (id: {agent.id})\")\n",
    "else:\n",
    "    agent = project_client.agents.create_agent(\n",
    "        model=model_name,\n",
    "        name=AGENT_NAME,\n",
    "        instructions=instructions,\n",
    "        toolset=toolset,\n",
    "    )\n",
    "    print(f\"creating agent > {agent.name} (id: {agent.id})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Conversation Thread\n",
    "In this step, we create a new conversation thread for the enterprise agent.\n",
    "Threads are used to manage and track conversations with the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread > created (id: thread_8g4b0MWnqjSw4z6phgTat9MD)\n"
     ]
    }
   ],
   "source": [
    "thread = project_client.agents.threads.create()\n",
    "print(f\"thread > created (id: {thread.id})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a Custom Event Handler\n",
    "Here, we define a custom event handler to manage logs and outputs for debugging.\n",
    "This handler will capture and display real-time events during the agent's operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyEventHandler(AgentEventHandler):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._current_message_id = None\n",
    "        self._accumulated_text = \"\"\n",
    "\n",
    "    def on_message_delta(self, delta: MessageDeltaChunk) -> None:\n",
    "        # If a new message id, start fresh\n",
    "        if delta.id != self._current_message_id:\n",
    "            # First, if we had an old message that wasn't completed, finish that line\n",
    "            if self._current_message_id is not None:\n",
    "                print()  # move to a new line\n",
    "            \n",
    "            self._current_message_id = delta.id\n",
    "            self._accumulated_text = \"\"\n",
    "            print(\"\\nassistant > \", end=\"\")  # prefix for new message\n",
    "\n",
    "        # Accumulate partial text\n",
    "        partial_text = \"\"\n",
    "        if delta.delta.content:\n",
    "            for chunk in delta.delta.content:\n",
    "                # partial_text += chunk.text.get(\"value\", \"\")\n",
    "                if isinstance(chunk, MessageDeltaTextContent):\n",
    "                    partial_text += chunk[\"text\"].get(\"value\", \"\")\n",
    "                elif isinstance(chunk, MessageDeltaImageFileContent):\n",
    "                    partial_text += chunk[\"image_file\"].get(\"file_id\", \"\")\n",
    "        self._accumulated_text += partial_text\n",
    "\n",
    "        # Print partial text with no newline\n",
    "        print(partial_text, end=\"\", flush=True)\n",
    "\n",
    "    def on_thread_message(self, message: ThreadMessage) -> None:\n",
    "        # When the assistant's entire message is \"completed\", print a final newline\n",
    "        if message.status == \"completed\" and message.role == \"assistant\":\n",
    "            print()  # done with this line\n",
    "            self._current_message_id = None\n",
    "            self._accumulated_text = \"\"\n",
    "        else:\n",
    "            # For other roles or statuses, you can log if you like:\n",
    "            print(f\"{message.status.name.lower()} (id: {message.id})\")\n",
    "\n",
    "    def on_thread_run(self, run: ThreadRun) -> None:\n",
    "        print(f\"status > {run.status.name.lower()}\")\n",
    "        if run.status == \"failed\":\n",
    "            print(f\"error > {run.last_error}\")\n",
    "\n",
    "    def on_run_step(self, step: RunStep) -> None:\n",
    "        print(f\"{step.type.name.lower()} > {step.status.name.lower()}\")\n",
    "\n",
    "    def on_run_step_delta(self, delta: RunStepDeltaChunk) -> None:\n",
    "        # If partial tool calls come in, we log them\n",
    "        if delta.delta.step_details and delta.delta.step_details.tool_calls:\n",
    "            for tcall in delta.delta.step_details.tool_calls:\n",
    "                if getattr(tcall, \"function\", None):\n",
    "                    if tcall.function.name is not None:\n",
    "                        print(f\"tool call > {tcall.function.name}\")\n",
    "\n",
    "    def on_unhandled_event(self, event_type: str, event_data):\n",
    "        print(f\"unhandled > {event_type} > {event_data}\")\n",
    "\n",
    "    def on_error(self, data: str) -> None:\n",
    "        print(f\"error > {data}\")\n",
    "\n",
    "    def on_done(self) -> None:\n",
    "        print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the Main Chat Functions\n",
    "These functions define how user messages and tool interactions are processed.\n",
    "It uses the agent's thread to handle conversations and streams partial responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_bing_query(request_url: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract the query string from something like:\n",
    "      https://api.bing.microsoft.com/v7.0/search?q=\"latest news about Microsoft January 2025\"\n",
    "    Returns: latest news about Microsoft January 2025\n",
    "    \"\"\"\n",
    "    match = re.search(r'q=\"([^\"]+)\"', request_url)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    # If no match, fall back to entire request_url\n",
    "    return request_url\n",
    "\n",
    "def extract_search_annotation(\n",
    "        annotation: MessageDeltaTextUrlCitationAnnotation, text_value_str: str) -> str:\n",
    "    \"\"\"\n",
    "    {'index': 0, 'type': 'text', 'text': {'value': '„Äê3:1‚Ä†Siemens fiscal report 2024„Äë', 'annotations': [{'index': 0, 'type': 'url_citation', 'text': '„Äê3:1‚Ä†Siemens fiscal report 2024„Äë', 'start_index': 1369, 'end_index': 1401, 'url_citation': {'url': 'doc_1', 'title': 'Siemens_Report_FY2024.pdf'}}]}}\n",
    "\n",
    "    {'index': 0, 'type': 'text', 'text': {'value': '„Äê14:0‚Ä†best_practices_lung_cancer.md„Äë', 'annotations': [{'index': 0, 'type': 'file_citation', 'text': '„Äê14:0‚Ä†best_practices_lung_cancer.md„Äë', 'start_index': 2486, 'end_index': 2522, 'file_citation': {'file_id': 'assistant-XfXFvez3N2CbpttpNb8MK4', 'quote': ''}}]}}\n",
    "    \"\"\"\n",
    "    if annotation[\"type\"] == \"url_citation\":\n",
    "        url = annotation[\"url_citation\"][\"url\"]\n",
    "        title = annotation[\"url_citation\"].get(\"title\", \"\")\n",
    "        start_idx = annotation.get(\"start_index\", 0)\n",
    "        end_idx = annotation.get(\"end_index\", 0)\n",
    "        return f\" [{text_value_str.strip()} {title}]({url}) ({start_idx}-{end_idx})\"\n",
    "    elif annotation[\"type\"] == \"file_citation\":\n",
    "        # file_id = annotation[\"file_citation\"][\"file_id\"]\n",
    "        file_id = \"\"\n",
    "        quote = annotation[\"file_citation\"].get(\"quote\", \"\")\n",
    "        title = annotation[\"text\"]\n",
    "        start_idx = annotation.get(\"start_index\", 0)\n",
    "        end_idx = annotation.get(\"end_index\", 0)\n",
    "        return f\" [{text_value_str.strip()} {title}]({file_id}) ({start_idx}-{end_idx})\"\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def convert_dict_to_chatmessage(msg: dict) -> ChatMessage:\n",
    "    \"\"\"\n",
    "    Convert a legacy dict-based message to a gr.ChatMessage.\n",
    "    Uses the 'metadata' sub-dict if present.\n",
    "    \"\"\"\n",
    "    return ChatMessage(\n",
    "        role=msg[\"role\"],\n",
    "        content=msg[\"content\"],\n",
    "        metadata=msg.get(\"metadata\", None)\n",
    "    )\n",
    "\n",
    "def azure_enterprise_chat(user_message: str, history: List[dict]):\n",
    "    \"\"\"\n",
    "    Accumulates partial function arguments into ChatMessage['content'], sets the\n",
    "    corresponding tool bubble status from \"pending\" to \"done\" on completion,\n",
    "    and also handles non-function calls like bing_grounding or file_search by appending a\n",
    "    \"pending\" bubble. Then it moves them to \"done\" once tool calls complete.\n",
    "\n",
    "    This function returns a list of ChatMessage objects directly (no dict conversion).\n",
    "    Your Gradio Chatbot should be type=\"messages\" to handle them properly.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert existing history from dict to ChatMessage\n",
    "    conversation = []\n",
    "    for msg_dict in history:\n",
    "        conversation.append(convert_dict_to_chatmessage(msg_dict))\n",
    "\n",
    "    # Append the user's new message\n",
    "    conversation.append(ChatMessage(role=\"user\", content=user_message))\n",
    "\n",
    "    # Immediately yield two outputs to clear the textbox\n",
    "    yield conversation, \"\"\n",
    "\n",
    "    # Post user message to the thread (for your back-end logic)\n",
    "    project_client.agents.messages.create(\n",
    "        thread_id=thread.id,\n",
    "        role=\"user\",\n",
    "        content=user_message\n",
    "    )\n",
    "\n",
    "    # Mappings for partial function calls\n",
    "    call_id_for_index: Dict[int, str] = {}\n",
    "    partial_calls_by_index: Dict[int, dict] = {}\n",
    "    partial_calls_by_id: Dict[str, dict] = {}\n",
    "    in_progress_tools: Dict[str, ChatMessage] = {}\n",
    "\n",
    "    # Titles for tool bubbles\n",
    "    function_titles = {\n",
    "        # \"fetch_weather\": \"‚òÅÔ∏è fetching weather\",\n",
    "        \"fetch_datetime\": \"üïí fetching datetime\",\n",
    "        # \"fetch_stock_price\": \"üìà fetching financial info\",\n",
    "        # \"send_email\": \"‚úâÔ∏è sending mail\",\n",
    "        \"file_search\": \"üìÑ searching docs\",\n",
    "        \"bing_grounding\": \"üîç searching bing\",\n",
    "        \"code_interpreter\": \"üìä analyzing data\",\n",
    "    }\n",
    "\n",
    "    def get_function_title(fn_name: str) -> str:\n",
    "        return function_titles.get(fn_name, f\"üõ† calling {fn_name}\")\n",
    "\n",
    "    def accumulate_args(storage: dict, name_chunk: str, arg_chunk: str):\n",
    "        \"\"\"Accumulates partial JSON data for a function call.\"\"\"\n",
    "        if name_chunk:\n",
    "            storage[\"name\"] += name_chunk\n",
    "        if arg_chunk:\n",
    "            storage[\"args\"] += arg_chunk\n",
    "\n",
    "    def finalize_tool_call(call_id: str):\n",
    "        \"\"\"Creates or updates the ChatMessage bubble for a function call.\"\"\"\n",
    "        if call_id not in partial_calls_by_id:\n",
    "            return\n",
    "        data = partial_calls_by_id[call_id]\n",
    "        fn_name = data[\"name\"].strip()\n",
    "        fn_args = data[\"args\"].strip()\n",
    "        if not fn_name:\n",
    "            return\n",
    "\n",
    "        if call_id not in in_progress_tools:\n",
    "            # Create a new bubble with status=\"pending\"\n",
    "            msg_obj = ChatMessage(\n",
    "                role=\"assistant\",\n",
    "                content=fn_args or \"\",\n",
    "                metadata={\n",
    "                    \"title\": get_function_title(fn_name),\n",
    "                    \"status\": \"pending\",\n",
    "                    \"id\": f\"tool-{call_id}\"\n",
    "                }\n",
    "            )\n",
    "            conversation.append(msg_obj)\n",
    "            in_progress_tools[call_id] = msg_obj\n",
    "        else:\n",
    "            # Update existing bubble\n",
    "            msg_obj = in_progress_tools[call_id]\n",
    "            msg_obj.content = fn_args or \"\"\n",
    "            msg_obj.metadata[\"title\"] = get_function_title(fn_name)\n",
    "\n",
    "    def upsert_tool_call(tcall: dict):\n",
    "        \"\"\"\n",
    "        1) Check the call type\n",
    "        2) If \"function\", gather partial name/args\n",
    "        3) If \"bing_grounding\" or \"file_search\", show a pending bubble\n",
    "        4) If \"code_interpreter\", create only one bubble\n",
    "        \"\"\"\n",
    "        t_type = tcall.get(\"type\", \"\")\n",
    "        call_id = tcall.get(\"id\", None)\n",
    "\n",
    "        # If call_id is None, generate a unique one (for parallel calls)\n",
    "        if call_id is None:\n",
    "            if t_type in (\"file_search\", \"bing_grounding\"):\n",
    "                call_id = str(uuid.uuid4())\n",
    "            elif t_type == \"code_interpreter\":\n",
    "                call_id = \"code_interpreter\"\n",
    "            elif t_type == \"azure_ai_search\":\n",
    "                call_id = \"azure_ai_search\"\n",
    "            else:\n",
    "                call_id = \"unknown_tool\"\n",
    "\n",
    "        # --- BING GROUNDING ---\n",
    "        if t_type == \"bing_grounding\":\n",
    "            request_url = tcall.get(\"bing_grounding\", {}).get(\"requesturl\", \"\")\n",
    "            if not request_url.strip():\n",
    "                return\n",
    "\n",
    "            query_str = extract_bing_query(request_url)\n",
    "            if not query_str.strip():\n",
    "                return\n",
    "\n",
    "            msg_obj = ChatMessage(\n",
    "                role=\"assistant\",\n",
    "                content=query_str,\n",
    "                metadata={\n",
    "                    \"title\": get_function_title(\"bing_grounding\"),\n",
    "                    \"status\": \"pending\",\n",
    "                    \"id\": f\"tool-{call_id}\" if call_id else \"tool-noid\"\n",
    "                }\n",
    "            )\n",
    "            conversation.append(msg_obj)\n",
    "            if call_id is not None:\n",
    "                in_progress_tools[call_id] = msg_obj\n",
    "            return\n",
    "\n",
    "        # --- FILE SEARCH ---\n",
    "        elif t_type == \"file_search\":\n",
    "            msg_obj = ChatMessage(\n",
    "                role=\"assistant\",\n",
    "                content=\"searching docs...\",\n",
    "                metadata={\n",
    "                    \"title\": get_function_title(\"file_search\"),\n",
    "                    \"status\": \"pending\",\n",
    "                    \"id\": f\"tool-{call_id}\" if call_id else \"tool-noid\"\n",
    "                }\n",
    "            )\n",
    "            conversation.append(msg_obj)\n",
    "            if call_id is not None:\n",
    "                in_progress_tools[call_id] = msg_obj\n",
    "            return\n",
    "        \n",
    "        # --- Azure AI SEARCH --- \n",
    "        elif t_type == \"azure_ai_search\":\n",
    "            if call_id not in in_progress_tools:\n",
    "                msg_obj = ChatMessage(\n",
    "                    role=\"assistant\",\n",
    "                    content=\"searching private index...\",\n",
    "                    metadata={\n",
    "                        \"title\": get_function_title(\"azure_ai_search\"),\n",
    "                        \"status\": \"pending\",\n",
    "                        \"id\": f\"tool-{call_id}\" if call_id else \"tool-noid\"\n",
    "                    }\n",
    "                )\n",
    "                conversation.append(msg_obj)\n",
    "                in_progress_tools[call_id] = msg_obj\n",
    "            return\n",
    "        \n",
    "        # -- CODE INTERPRETER ---\n",
    "        elif t_type == \"code_interpreter\":\n",
    "            if call_id not in in_progress_tools:\n",
    "                msg_obj = ChatMessage(\n",
    "                    role=\"assistant\",\n",
    "                    content=\"analyzing data...\",\n",
    "                    metadata={\n",
    "                        \"title\": get_function_title(\"code_interpreter\"),\n",
    "                        \"status\": \"pending\",\n",
    "                        \"id\": f\"tool-{call_id}\"\n",
    "                    }\n",
    "                )\n",
    "                conversation.append(msg_obj)\n",
    "                in_progress_tools[call_id] = msg_obj\n",
    "            return\n",
    "\n",
    "        # --- NON-FUNCTION CALLS ---\n",
    "        elif t_type != \"function\":\n",
    "            return\n",
    "\n",
    "        # --- FUNCTION CALL PARTIAL-ARGS ---\n",
    "        index = tcall.get(\"index\")\n",
    "        new_call_id = call_id\n",
    "        fn_data = tcall.get(\"function\", {})\n",
    "        name_chunk = fn_data.get(\"name\", \"\")\n",
    "        arg_chunk = fn_data.get(\"arguments\", \"\")\n",
    "\n",
    "        if new_call_id:\n",
    "            call_id_for_index[index] = new_call_id\n",
    "\n",
    "        call_id = call_id_for_index.get(index)\n",
    "        if not call_id:\n",
    "            # Accumulate partial\n",
    "            if index not in partial_calls_by_index:\n",
    "                partial_calls_by_index[index] = {\"name\": \"\", \"args\": \"\"}\n",
    "            accumulate_args(partial_calls_by_index[index], name_chunk, arg_chunk)\n",
    "            return\n",
    "\n",
    "        if call_id not in partial_calls_by_id:\n",
    "            partial_calls_by_id[call_id] = {\"name\": \"\", \"args\": \"\"}\n",
    "\n",
    "        if index in partial_calls_by_index:\n",
    "            old_data = partial_calls_by_index.pop(index)\n",
    "            partial_calls_by_id[call_id][\"name\"] += old_data.get(\"name\", \"\")\n",
    "            partial_calls_by_id[call_id][\"args\"] += old_data.get(\"args\", \"\")\n",
    "\n",
    "        # Accumulate partial\n",
    "        accumulate_args(partial_calls_by_id[call_id], name_chunk, arg_chunk)\n",
    "\n",
    "        # Create/update the function bubble\n",
    "        finalize_tool_call(call_id)\n",
    "\n",
    "    # -- EVENT STREAMING --\n",
    "    with project_client.agents.runs.stream(\n",
    "        thread_id=thread.id,\n",
    "        agent_id=agent.id,\n",
    "        # assistant_id=agent.id,\n",
    "        event_handler=MyEventHandler(),  # the event handler handles console output\n",
    "        parallel_tool_calls=True,\n",
    "        tool_choice=AgentsNamedToolChoice(\n",
    "            type=AgentsNamedToolChoiceType.CODE_INTERPRETER), \n",
    "    ) as stream:\n",
    "        # pulling the result from the stream manually\n",
    "        for item in stream:\n",
    "            event_type, event_data, *_ = item\n",
    "\n",
    "            # Remove any None items that might have been appended\n",
    "            conversation = [m for m in conversation if m is not None]\n",
    "\n",
    "            # 1) Partial tool calls\n",
    "            if event_type == \"thread.run.step.delta\":\n",
    "                step_delta = event_data.get(\"delta\", {}).get(\"step_details\", {})\n",
    "                if step_delta.get(\"type\") == \"tool_calls\":\n",
    "                    for tcall in step_delta.get(\"tool_calls\", []):\n",
    "                        upsert_tool_call(tcall)\n",
    "                    yield conversation, \"\"\n",
    "\n",
    "            # 2) run_step\n",
    "            elif event_type == \"run_step\":\n",
    "                step_type = event_data[\"type\"]\n",
    "                step_status = event_data[\"status\"]\n",
    "\n",
    "                # If tool calls are in progress, new or partial\n",
    "                if step_type == \"tool_calls\" and step_status == \"in_progress\":\n",
    "                    for tcall in event_data[\"step_details\"].get(\"tool_calls\", []):\n",
    "                        upsert_tool_call(tcall)\n",
    "                    yield conversation, \"\"\n",
    "\n",
    "                elif step_type == \"tool_calls\" and step_status == \"completed\":\n",
    "                    for cid, msg_obj in in_progress_tools.items():\n",
    "                        msg_obj.metadata[\"status\"] = \"done\"\n",
    "                    in_progress_tools.clear()\n",
    "                    partial_calls_by_id.clear()\n",
    "                    partial_calls_by_index.clear()\n",
    "                    call_id_for_index.clear()\n",
    "                    yield conversation, \"\"\n",
    "\n",
    "                elif step_type == \"message_creation\" and step_status == \"in_progress\":\n",
    "                    msg_id = event_data[\"step_details\"][\"message_creation\"].get(\"message_id\")\n",
    "                    if msg_id:\n",
    "                        conversation.append(ChatMessage(role=\"assistant\", content=\"\"))\n",
    "                    yield conversation, \"\"\n",
    "\n",
    "                elif step_type == \"message_creation\" and step_status == \"completed\":\n",
    "                    yield conversation, \"\"\n",
    "\n",
    "            # 3) partial text from the assistant\n",
    "            elif event_type == \"thread.message.delta\":\n",
    "                agent_msg = \"\"\n",
    "                for chunk in event_data[\"delta\"][\"content\"]:\n",
    "                    # print(\"chunk > \", chunk)\n",
    "                    # print(\"chunk type > \", type(chunk))\n",
    "                    if isinstance(chunk, MessageDeltaTextContent):\n",
    "                        # Safely get the text value\n",
    "                        text_obj: MessageDeltaTextContentObject = chunk.get(\"text\", {})\n",
    "                        text_value_str = text_obj.get(\"value\", \"\")\n",
    "                        annotations = text_obj.get(\"annotations\", None)\n",
    "                        if annotations:\n",
    "                            # Extract the URL citation if available\n",
    "                            for annotation in annotations: \n",
    "                                agent_msg += extract_search_annotation(annotation, text_value_str)\n",
    "                        else:\n",
    "                            agent_msg += text_value_str\n",
    "                            \n",
    "                    elif isinstance(chunk, MessageDeltaImageFileContent):\n",
    "                        file_id = chunk[\"image_file\"].get(\"file_id\", \"\")\n",
    "                        byte_stream = project_client.agents.files.get_content(file_id=file_id)\n",
    "                        # Encode to base64\n",
    "                        # Join all bytes from the iterator\n",
    "                        image_bytes = b\"\".join(byte_stream)\n",
    "                        b64_image = base64.b64encode(image_bytes).decode(\"utf-8\")\n",
    "                        # Use Markdown to display the image in Gradio\n",
    "                        agent_msg += f\"![image](data:image/png;base64,{b64_image})\"  \n",
    "\n",
    "                message_id = event_data[\"id\"]\n",
    "\n",
    "                # Try to find a matching assistant bubble\n",
    "                matching_msg = None\n",
    "                for msg in reversed(conversation):\n",
    "                    if msg.metadata and msg.metadata.get(\"id\") == message_id and msg.role == \"assistant\":\n",
    "                        matching_msg = msg\n",
    "                        break\n",
    "\n",
    "                if matching_msg:\n",
    "                    # Append newly streamed text\n",
    "                    matching_msg.content += agent_msg\n",
    "                else:\n",
    "                    # Append to last assistant or create new\n",
    "                    if (\n",
    "                        not conversation\n",
    "                        or conversation[-1].role != \"assistant\"\n",
    "                        or (\n",
    "                            conversation[-1].metadata\n",
    "                            and str(conversation[-1].metadata.get(\"id\", \"\")).startswith(\"tool-\")\n",
    "                        )\n",
    "                    ):\n",
    "                        conversation.append(ChatMessage(role=\"assistant\", content=agent_msg))\n",
    "                    else:\n",
    "                        conversation[-1].content += agent_msg\n",
    "\n",
    "                yield conversation, \"\"\n",
    "\n",
    "            # 4) If entire assistant message is completed\n",
    "            elif event_type == \"thread.message\":\n",
    "                if event_data[\"role\"] == \"assistant\" and event_data[\"status\"] == \"completed\":\n",
    "                    for cid, msg_obj in in_progress_tools.items():\n",
    "                        msg_obj.metadata[\"status\"] = \"done\"\n",
    "                    in_progress_tools.clear()\n",
    "                    partial_calls_by_id.clear()\n",
    "                    partial_calls_by_index.clear()\n",
    "                    call_id_for_index.clear()\n",
    "                    yield conversation, \"\"\n",
    "\n",
    "            # 5) Final done\n",
    "            elif event_type == \"thread.message.completed\":\n",
    "                for cid, msg_obj in in_progress_tools.items():\n",
    "                    msg_obj.metadata[\"status\"] = \"done\"\n",
    "                in_progress_tools.clear()\n",
    "                partial_calls_by_id.clear()\n",
    "                partial_calls_by_index.clear()\n",
    "                call_id_for_index.clear()\n",
    "                yield conversation, \"\"\n",
    "                break\n",
    "\n",
    "    return conversation, \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Gradio UI\n",
    "Create a Gradio interface for interacting with the enterprise agent. \n",
    "Include a chatbot component and a text input box for user queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note:\n",
      "tool calling may fail, close the chat with trash bin icon (üóëÔ∏è) and rerun the prompt.\n",
      "\n",
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "brand_theme = gr.themes.Default(\n",
    "    primary_hue=\"blue\",\n",
    "    secondary_hue=\"blue\",\n",
    "    neutral_hue=\"gray\",\n",
    "    font=[\"Segoe UI\", \"Arial\", \"sans-serif\"],\n",
    "    font_mono=[\"Courier New\", \"monospace\"],\n",
    "    text_size=\"lg\",\n",
    ").set(\n",
    "    button_primary_background_fill=\"#0f6cbd\",\n",
    "    button_primary_background_fill_hover=\"#115ea3\",\n",
    "    button_primary_background_fill_hover_dark=\"#4f52b2\",\n",
    "    button_primary_background_fill_dark=\"#5b5fc7\",\n",
    "    button_primary_text_color=\"#ffffff\",\n",
    "    button_secondary_background_fill=\"#e0e0e0\",\n",
    "    button_secondary_background_fill_hover=\"#c0c0c0\",\n",
    "    button_secondary_background_fill_hover_dark=\"#a0a0a0\",\n",
    "    button_secondary_text_color=\"#000000\",\n",
    "    body_background_fill=\"#f5f5f5\",\n",
    "    block_background_fill=\"#ffffff\",\n",
    "    body_text_color=\"#242424\",\n",
    "    body_text_color_subdued=\"#616161\",\n",
    "    block_border_color=\"#d1d1d1\",\n",
    "    block_border_color_dark=\"#333333\",\n",
    "    input_background_fill=\"#ffffff\",\n",
    "    input_border_color=\"#d1d1d1\",\n",
    "    input_border_color_focus=\"#0f6cbd\",\n",
    ")\n",
    "\n",
    "with gr.Blocks(theme=brand_theme, css=\"footer {visibility: hidden;}\", fill_height=True) as demo:\n",
    "\n",
    "    def clear_thread():\n",
    "        global thread\n",
    "        thread = project_client.agents.threads.create()\n",
    "        return []\n",
    "\n",
    "    def on_example_clicked(evt: gr.SelectData):\n",
    "        return evt.value[\"text\"]  # Fill the textbox with that example text\n",
    "\n",
    "    gr.HTML(\"<h1 style=\\\"text-align: center;\\\">Azure AI Agent Service</h1>\")\n",
    "    \n",
    "    # patient 10\n",
    "    # Cough and wheezing for 5 months.\n",
    "    # You need to \"continue\" if the Agent generate a plan.\n",
    "    chatbot = gr.Chatbot(\n",
    "        type=\"messages\",\n",
    "        examples=[\n",
    "            # {\"text\": \"Patient 10 has\\n1.) The aorta is atherosclerotic.\\n2.) A 4.5 cm mass is in the left lower lobe.\\n3.) There are chronic healed bilateral rib fractures.\\n4.) The lungs are hyperinflated.The heart size is normal.\"},\n",
    "            {\"text\": \"Is patient with PATIENT_ID 10 an outlier focusing on symptoms and risk factors from the lung_cancer_patient_clinical_features table, Use LOF and generate a boxplot chart for all normalized features?\"},\n",
    "            {\"text\": \"Suggest a treatment plan with research innovation from best practices for patient with PATIENT_ID 10.\"},\n",
    "        ],\n",
    "        show_label=False,\n",
    "        scale=1,\n",
    "    )\n",
    "\n",
    "    textbox = gr.Textbox(\n",
    "        show_label=False,\n",
    "        lines=1,\n",
    "        submit_btn=True,\n",
    "    )\n",
    "\n",
    "    # Populate textbox when an example is clicked\n",
    "    chatbot.example_select(fn=on_example_clicked, inputs=None, outputs=textbox)\n",
    "\n",
    "    # On submit: call azure_enterprise_chat, then clear the textbox\n",
    "    (textbox\n",
    "     .submit(\n",
    "         fn=azure_enterprise_chat,\n",
    "         inputs=[textbox, chatbot],\n",
    "         outputs=[chatbot, textbox],\n",
    "     )\n",
    "     .then(\n",
    "         fn=lambda: \"\",\n",
    "         outputs=textbox,\n",
    "     )\n",
    "    )\n",
    "\n",
    "    # A \"Clear\" button that resets the thread and the Chatbot\n",
    "    chatbot.clear(fn=clear_thread, outputs=chatbot)\n",
    "\n",
    "# Launch your Gradio app\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Note:\\ntool calling may fail, close the chat with trash bin icon (üóëÔ∏è) and rerun the prompt.\\n\")\n",
    "    demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) delete agent, thread, and vector store resources\n",
    "Uncomment out the next cell block to delete the resources created in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from azure.identity import DefaultAzureCredential\n",
    "# from azure.ai.projects import AIProjectClient\n",
    "# import os\n",
    "\n",
    "# credential = DefaultAzureCredential()\n",
    "# project_client_delete = AIProjectClient(\n",
    "#    credential=credential,\n",
    "#    endpoint=os.environ.get(\"PROJECT_ENDPOINT\")\n",
    "# )\n",
    "\n",
    "# try:\n",
    "#    project_client_delete.agents.delete_agent(agent.id)\n",
    "#    print(\"Agent deletion successful.\")\n",
    "#    project_client_delete.agents.threads.delete(thread.id)\n",
    "#    print(\"Thread deletion successful.\")\n",
    "#    project_client_delete.agents.vector_stores.delete(vector_store_id)\n",
    "#    print(\"Vector store deletion successful.\")\n",
    "#    print(\"All deletions succeeded.\")\n",
    "# except Exception as e:\n",
    "#    print(f\"Error during deletion: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "status > queued\n",
      "status > queued\n",
      "status > in_progress\n",
      "tool_calls > in_progress\n",
      "tool_calls > in_progress\n",
      "tool_calls > completed\n",
      "tool_calls > in_progress\n",
      "tool_calls > in_progress\n",
      "tool_calls > completed\n",
      "tool_calls > in_progress\n",
      "tool_calls > in_progress\n",
      "tool_calls > completed\n",
      "message_creation > in_progress\n",
      "message_creation > in_progress\n",
      "in_progress (id: msg_KXEAL73l4k8HcjlWyqFoWd3c)\n",
      "in_progress (id: msg_KXEAL73l4k8HcjlWyqFoWd3c)\n",
      "\n",
      "assistant > assistant-GsMFbmZF7Bta3jtkSwRRczThe patient with PATIENT_ID 10 is not classified as an outlier based on the Local Outlier Factor (LOF) analysis on symptoms and risk factors features from the lung_cancer_patient_clinical_features table. The LOF score for this patient is approximately 1.00, which indicates typical behavior rather than being an outlier.\n",
      "\n",
      "I have also generated a boxplot of all the normalized features used in the analysis, which visually represents the distribution and spread of the standardized clinical features among the patients.\n",
      "\n",
      "If you would like, I can provide more detailed information about the patient's feature values or the outlier analysis results.\n",
      "status > queued\n",
      "status > queued\n",
      "status > in_progress\n",
      "tool_calls > in_progress\n",
      "tool_calls > in_progress\n",
      "tool_calls > completed\n",
      "tool_calls > in_progress\n",
      "tool_calls > in_progress\n",
      "tool_calls > completed\n",
      "tool_calls > in_progress\n",
      "tool_calls > in_progress\n",
      "tool_calls > completed\n",
      "message_creation > in_progress\n",
      "message_creation > in_progress\n",
      "in_progress (id: msg_BYmUknHSwu1JdcEmkjmaNuGy)\n",
      "in_progress (id: msg_BYmUknHSwu1JdcEmkjmaNuGy)\n",
      "\n",
      "assistant > I currently do not have direct access to an internal knowledge base search tool but can assist by searching the uploaded files if they contain relevant best practice guidelines or treatment innovation data.\n",
      "\n",
      "I will search for \"treatment\" and \"lung cancer\" related terms in the uploaded documents to find best practices and innovative treatments suitable for patient 10's profile.\n",
      "status > queued\n",
      "status > queued\n",
      "status > in_progress\n",
      "tool_calls > in_progress\n",
      "tool_calls > in_progress\n",
      "tool_calls > completed\n",
      "tool_calls > in_progress\n",
      "tool_calls > in_progress\n",
      "tool_calls > completed\n",
      "message_creation > in_progress\n",
      "message_creation > in_progress\n",
      "in_progress (id: msg_C0dzsGxCgLMGjRtQd1ZsHr3Y)\n",
      "in_progress (id: msg_C0dzsGxCgLMGjRtQd1ZsHr3Y)\n",
      "\n",
      "assistant > For patient with PATIENT_ID 10, a 35-year-old female who smokes and has multiple risk factors (yellow fingers, anxiety, peer pressure, allergy, alcohol consumption, fatigue) but no current lung cancer diagnosis, the suggested treatment plan with research innovation from best practices includes:\n",
      "\n",
      "1. Smoking Cessation:\n",
      "   - Essential to counsel and support through behavioral therapy, nicotine replacement, or pharmacotherapy to reduce lung cancer risk.\n",
      "\n",
      "2. Early Detection and Monitoring:\n",
      "   - Enroll in regular lung cancer screening programs using low-dose CT scans to detect lung cancer at an early and potentially curable stage given the smoking history and symptoms.\n",
      "\n",
      "3. Symptomatic and Comorbidity Management:\n",
      "   - Manage anxiety and peer-related psychological stress with counseling or therapy.\n",
      "   - Treat allergies and fatigue with appropriate medications and lifestyle interventions.\n",
      "   - Counsel on reducing alcohol consumption and promote overall healthy lifestyle changes.\n",
      "\n",
      "4. Multidisciplinary Care Approach:\n",
      "   - Involve a team comprising primary care, pulmonology, psychology, and potentially oncology specialists to provide personalized and comprehensive care.\n",
      "\n",
      "5. Research Innovations and Clinical Trials:\n",
      "   - Consider referral to clinical trials focusing on lung cancer prevention, novel smoking cessation techniques, immunomodulatory therapies, or interventions addressing psychological stress in high-risk patients.\n",
      "\n",
      "6. Follow-up and Support:\n",
      "   - Ensure regular follow-up for early detection of any disease progression and manage health holistically.\n",
      "\n",
      "This plan aligns with current best practices emphasizing smoking cessation, early detection, multidisciplinary management, and integration of innovative clinical trial options for prevention and supportive care, as outlined by major oncology guidelines and research consensus„Äê20:0‚Ä†best_practices_lung_cancer.md„Äë.\n"
     ]
    }
   ],
   "source": [
    "# existing_stores = project_client_delete.agents.vector_stores.list()\n",
    "# for store in existing_stores:\n",
    "#     project_client_delete.agents.vector_stores.delete(store.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azfdydemo3.12pip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
