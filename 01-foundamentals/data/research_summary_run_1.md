Final Report:
# Latest Advancements in Quantum Computing (Past Year)

## Hardware Developments

- **Scaling Qubits and Performance:** IBM achieved major hardware milestones with its new **Heron R2** superconducting processor. Debuted in late 2024 with 156 qubits, Heron R2 can execute up to **5,000 two-qubit gate operations** in a single circuit – nearly double the previous generation’s capacity【57:9†source】. This improvement, along with better coherence and gate fidelity, led to a **50× speedup** on complex workloads (e.g. cutting a 120-hour computation down to ~2.4 hours)【57:9†source】,【57:9†source】. IBM’s focus has shifted from just qubit count to **quality and usable circuit depth**, enabling more meaningful quantum simulations in domains like materials science and chemistry. Its roadmap remains ambitious, aiming for error-corrected systems by 2029 (with a ~1,121-qubit chip named *Condor* and multi-chip modules in development)【57:5†source】,.

- **Breakthroughs in Error Reduction (Google):** *Google’s Quantum AI* team unveiled the **“Willow”** quantum chip in December 2024, marking a **breakthrough in error correction**. Willow demonstrated that as the number of qubits in a logical encoding grows, the overall error rate *drops exponentially* – a landmark achievement known as operating *“below the threshold”* for error correction【57:3†source】. Google scaled up surface-code error-correction from a 3×3 to a 7×7 qubit grid, **halving the error rate with each increase**【57:3†source】. The Willow processor (105 qubits) also set a performance record in a standard random circuit sampling test, completing a computation in under 5 minutes that would have taken a top supercomputer an estimated 10^25 years【57:3†source】,【57:3†source】. This dramatic 10-septillion-year speedup showcases a **beyond-classical capability**, reinforcing Google’s claim of quantum advantage at an unprecedented scale. The achievement – published in *Nature* – provides the clearest evidence yet that larger quantum computers *can get more accurate as they grow*, paving the way to useful, fault-tolerant machines,【57:3†source】.

- **Topological Qubits (Microsoft):** *Microsoft* announced a significant hardware milestone with its **“Majorana 1”** chip, pursuing a topological approach to quantum computing【57:5†source】,【57:5†source】. This innovative processor is built using **Majorana quasi-particles** – exotic states of matter that encode information non-locally – to create qubits inherently protected from certain errors. By integrating *topological superconductors* (dubbed “**topoconductors**”), the Majorana 1 aims to achieve much higher qubit stability **without relying on heavy error-correction overhead**【57:5†source】. Microsoft’s CEO Satya Nadella touted this as *“an entirely new state of matter”* enabling the first quantum processing unit with a topological core【57:5†source】. If validated, this could dramatically expedite scaling (Microsoft envisions a **million-qubit** topological quantum computer in reach)【57:5†source】. However, the approach remains in early experimental stages, and some in the community are cautious about the claims until further peer-reviewed evidence is available【57:5†source】.

- **Advances in Trapped-Ion Systems:** Trapped-ion quantum computers also saw notable progress. **Quantinuum (Honeywell)** upgraded its flagship ion-trap system to **56 qubits** in 2024, achieving all-to-all connectivity and high fidelity on each additional qubit , . In a joint effort with Microsoft, Quantinuum demonstrated it could create **four fully error-corrected logical qubits** from 32 physical ions, with logical error rates up to *800× lower* than physical qubits . Meanwhile, **IonQ** announced an *accelerated roadmap* leveraging new chip-integrated traps and photonic interconnects (bolstered by its acquisitions of Oxford Ionics and Lightspeed). The company projects a leap from its current ~AQ\[algorithmic qubit\]  # <sub>currently ~# of high-fidelity qubits</sub> to **~20,000 physical qubits by 2028**, spread across modularly networked ion-trap chips,. This would correspond to roughly **1,600 error-corrected logical qubits** (if achieved), – potentially making IonQ the first to reach a *cryptographically relevant* quantum computer capable of breaking some encryption. Such goals far exceed prior targets and underscore a race toward scale by late 2020s. On the R&D front, academic groups (e.g. at Sandia, UMD, and Cornell) have prototyped new ion-trap chips capable of holding **100–200 ions with parallel gate operations**, improving prospects for scaling ion-based hardware in coming years.

- **Alternative Technologies – Photonic & Annealing:** Other paradigms also advanced. **Photonic quantum computing** startups like *PsiQuantum* and *Xanadu* continued developing optical qubit systems; PsiQuantum, for instance, has been aiming for a million-photon qubit machine by 2027–2028 (though its results remain mostly behind closed doors). **Neutral-atom quantum computers** (e.g. by Pasqal and QuEra) expanded the size of their atomic arrays and improved gate fidelities, indicating progress toward programmable analog and digital simulations with 100s of atoms. In the realm of quantum annealing, **D-Wave Systems** introduced its **Advantage2** annealer with over **5,000 superconducting qubits**. This new annealing machine (marking D-Wave’s 25-year anniversary in quantum tech) offers higher connectivity and lower noise, targeting complex optimization problems. While annealing qubits are not directly comparable to gate-based qubits, D-Wave’s continued hardware improvements have increased the size of solvable problems and inspired hybrid algorithms combining annealers with classical solvers. Overall, the past year saw **quantum hardware at its largest and most reliable to date**, with multiple platforms hitting milestones in qubit count *and* fidelity – critical steps on the path toward practical quantum advantage.

## Algorithmic Progress

- **Fault-Tolerant Quantum Error Correction:** Tremendous strides were made in quantum error-correction (QEC) research. Google’s above-mentioned *Willow* chip provided the first experimental evidence that adding more qubits in a QEC code can **reduce the overall error rate** (an exponential suppression of errors)【57:3†source】. This is a milestone that the field had pursued for nearly 30 years, finally showing a *“below-threshold”* error-correction regime in a real system【57:3†source】. In practice, Google implemented larger surface code cycles (up to 49 physical qubits encoding 1 logical qubit) and observed that the logical qubit’s fidelity *improved* as code size grew – a clear indicator that **error-corrected qubits can outperform uncorrected ones**【57:3†source】,【57:3†source】. Similarly, Microsoft and Quantinuum demonstrated end-to-end fault tolerance on a small scale: using four logical qubits encoded across Quantinuum’s ion traps, they achieved an **error rate 800× lower** than the corresponding physical qubits【57:1†source】, . Remarkably, they ran **14,000 operations with no error** on these logical qubits【57:1†source】. This was heralded as moving from “NISQ” noisy computing to a **“Level 2 Resilient”** era of quantum computing【57:1†source】,【57:1†source】. These breakthroughs indicate that **quantum error correction is finally working in practice**, setting the stage for scaling up logical qubit counts. Researchers also introduced techniques like **“active syndrome extraction”** (detecting and correcting errors on the fly without disturbing the qubits)【57:1†source】 – crucial for sustaining computations long enough to solve real problems.

- **New Quantum Algorithms & Complexity Insights:** Beyond error correction, the past year saw the development of novel algorithms and refinements of existing ones. A notable theoretical advance was the extension of the **Quantum Singular Value Transformation** framework to directly handle eigenvalue calculations for non-normal matrices【57:4†source】. This result, published by leading quantum algorithm researchers, opens the door to more efficient quantum algorithms for solving linear systems and eigenproblems – fundamental tasks in scientific computing【57:4†source】. In cryptography, while no *practical* quantum algorithm has yet broken modern encryption, researchers continued to optimize algorithms like Shor’s and Grover’s, and discovered more quantum-friendly schemes for tasks such as solving Pell’s equation and lattice problems (feeding into post-quantum cryptography research). Notably, a **classical** breakthrough reminded the community of the moving target for quantum advantage: in 2024, computer scientists developed a new classical algorithm that could simulate certain quantum boson-sampling experiments more efficiently, undermining some prior claims of quantum supremacy in that area【57:4†source】. This prompted fresh work on harder quantum sampling tasks and tighter complexity proofs. Overall, the **frontier of quantum algorithms expanded**, covering domains from solving differential equations and open quantum system dynamics【57:8†source】 to quantum machine learning. For example, researchers proposed improved variational algorithms for simulating chemical dynamics with fewer qubits, and new error-aware algorithms for combinatorial optimization on NISQ devices.

- **Hybrid and Variational Algorithms:** With near-term hardware limitations, **hybrid quantum-classical algorithms** have been a focal point. In 2024, there were refinements to techniques like the **Variational Quantum Eigensolver (VQE)** and **Quantum Approximate Optimization Algorithm (QAOA)**. Researchers introduced adaptive variational methods that build problem-specific ansätze on the fly, which achieved better accuracy for simulating quantum dynamics (e.g. via adaptive operator pools for Lindblad equations)【57:8†source】. Hybrid algorithms that delegate parts of the workload to classical processors also grew more sophisticated. One highlight was new **quantum optimization algorithms for Rydberg-atom processors**: by leveraging the natural atom interactions, these algorithms solved certain graph problems with fewer steps than classical heuristics. Early results hint at quantum *heuristic* advantage in areas like maximum independent set and scheduling using Rydberg tweezer arrays. Meanwhile, hybrid classical-quantum workflows were demonstrated for engineering applications (as detailed in the next section), underscoring how **quantum subroutines can accelerate traditional computing tasks**. Such developments illustrate a trend toward *“quantum-inspired”* solutions and deeper integration of quantum accelerators into classical HPC environments.

- **Software Tools and Frameworks:** The year also brought progress in **quantum software ecosystems** and developer tools. IBM released major upgrades to its open-source **Qiskit** platform, which independent benchmarks identified as the **most reliable and high-performing quantum SDK** in over 1,000 tests【57:2†source】. New Qiskit services include an AI-powered circuit optimizer and a code assistant for generating quantum programs using *generative AI models*【57:2†source】,【57:2†source】. These help automate the crafting and transpilation of complex circuits. Moreover, IBM and partners introduced libraries for **error mitigation** that dramatically extend useful circuit depth. For instance, an error-mitigation technique based on tensor networks (offered by startup Algorithmiq) can filter out noise from results in post-processing, allowing accurate execution of circuits with up to 5,000 entangled gates【57:2†source】. This method was used in IBM’s 2024 utility experiments and helped achieve the 50× speedup on the Heron processor by reducing the number of runs needed【57:9†source】,【57:9†source】. On the industrial side, Cloud providers (Amazon Braket, Azure Quantum, etc.) expanded their algorithm libraries and supported larger hybrid jobs, while open-source frameworks like **TorchQuantum** and **TensorFlow Quantum** integrated more seamlessly with machine learning toolkits. Notably, researchers have started using deep learning to optimize quantum control – *TorchQC*, as one example, applies neural networks to tune pulse sequences and was used to improve gate fidelities on certain hardware. This convergence of AI and quantum software is **optimizing experiment design and error calibration**, effectively squeezing more algorithmic performance out of today’s devices. The past year’s software advances, from better compilers to domain-specific quantum libraries, are lowering barriers for developers and enabling more complex algorithmic demonstrations on real hardware.

## Application Innovations

- **Chemistry and Materials Science:** Quantum computing made tangible inroads into simulating chemical systems that are intractable for classical computers. In a collaboration between IBM and research partners, a **quantum-centric simulation** was used to model the electronic structure of an iron–sulfur molecular complex (a challenging problem in quantum chemistry)【57:2†source】,【57:2†source】. By combining an IBM Quantum System One with classical HPC for post-processing, researchers at RIKEN (Japan) accurately calculated properties of an iron-sulfur cluster (Fe–S) that plays a role in biological and catalytic processes【57:2†source】. Similarly, at the Cleveland Clinic, scientists leveraged an on-site IBM quantum computer to study **intermolecular interactions** – specifically, they examined *non-covalent bonding* between drug molecules and target proteins using quantum simulations【57:2†source】. This marked the **first time a quantum computer has been used to model drug–target binding** dynamics with real hardware【57:2†source】. While these molecular simulations are still small in scale, they point to quantum computers’ potential in drug discovery and materials design. Indeed, a 2025 report noted that quantum algorithms have now computed single-point energy profiles of medium-sized molecules with **unprecedented accuracy**, thanks to better error mitigation. Quantum chemistry applications are expected to accelerate discovery of new pharmaceuticals, catalysts, and superconducting materials as quantum hardware grows. Companies like **QC Ware and BMW** have also used quantum chemistry algorithms to investigate battery materials and corrosion chemistry, seeking improved materials for energy storage and automotive design. These early successes in chemically relevant calculations underscore quantum computing’s promise in *materials science*, potentially revolutionizing how we design molecules by allowing direct simulation of quantum interactions.

- **Engineering & Optimization (Quantum-Assisted Simulations):** A landmark application result came in the field of engineering simulation. In March 2025, **IonQ and Ansys** announced one of the first documented cases of a **quantum speedup in a real-world engineering workflow**. They integrated IonQ’s *Forte* trapped-ion quantum processor into Ansys’s **fluid dynamics simulation** for a blood pump design. The quantum chip was used to solve a hard **graph optimization** sub-problem (related to fluid flow mesh partitioning) within the Ansys **LS-DYNA** software. The hybrid quantum-classical approach achieved a **12% reduction in overall simulation time** compared to a purely classical run,. In this case, the model involved ~2.6 million vertices and 40 million edges representing the fluid’s mesh – a highly complex system. By offloading an optimization routine to the quantum computer, the simulation finished faster, marking a *practical* performance gain. This is significant because it demonstrates quantum computing’s value in **computational fluid dynamics (CFD)** and potentially other heavy computations (like crash simulations, as the team noted). The result, though modest in percentage, is arguably **the first quantum advantage shown for an engineering problem**. It suggests that near-term quantum processors, when used judiciously in a hybrid mode, can *accelerate* high-performance computing tasks in aerospace, automotive, and manufacturing industries. Similarly, in logistics and supply chain optimization, companies have tested quantum solvers on routing and scheduling problems. For example, VW and BMW have previously collaborated with quantum providers to optimize factory robot paths and traffic flows; such projects are now evolving with better quantum hardware. These endeavors indicate that **quantum optimization is moving from toy problems to real industrial use-cases**.

- **Finance and Economics:** The finance sector has been increasingly active in quantum computing research, aiming to exploit quantum algorithms for complex economic problems. Over the past year, several financial institutions initiated pilots and partnerships. Notably, **Citi’s Innovation Labs** partnered with quantum startup **Classiq** to explore **portfolio optimization** on quantum hardware. This involves using quantum algorithms (and Classiq’s circuit synthesis tools) to find optimal asset allocations more efficiently than classical heuristics. Likewise, **Mizuho Bank** and other banks in Japan and Europe have been testing quantum algorithms for risk analysis, option pricing, and arbitrage opportunities. While quantum computers are not yet outpacing classical methods in finance, prototypes of *quantum Monte Carlo* for pricing derivatives and *quantum optimization* for clearing networks have shown promise in scaling better with problem size. In one case, a quantum annealer was used by Spanish bank BBVA to optimize credit portfolio risk, yielding results comparable to classical algorithms. Furthermore, quantum-inspired algorithms (running on classical hardware but designed using quantum principles) have delivered improvements in scenario analysis for some hedge funds,. On the **cryptography side**, financial firms are deeply concerned with *post-quantum security* (as large-scale quantum computers could break RSA and ECC encryption). This has driven early adoption of **post-quantum cryptography (PQC)** in financial networks – for instance, JPMorgan implemented quantum-safe communication links for inter-bank data as a precautionary measure. Overall, in finance *and* global economics, **quantum computing is seen as both an opportunity and a threat**: an opportunity for superior optimization and simulation of markets, and a threat to current cryptographic infrastructure. This is spurring investment in quantum R&D across major banks, with the expectation that financial advantage will come to those who master quantum algorithms first.

- **Cybersecurity and Cryptography:** Given the rapid advancements in quantum hardware, there’s been a parallel push to secure data against future quantum decryption. A major milestone occurred in August 2024 when **NIST released its first set of Post-Quantum Cryptography standards** – encryption and digital signature algorithms designed to be resistant to quantum attacks【57:7†source】,【57:7†source】. These standards include lattice-based schemes (like **CRYSTALS-Kyber** for encryption and **Dilithium** for signatures) and hash-based signatures (e.g. SPHINCS+)【57:7†source】. They have now been formalized as FIPS 203, 204, 205, etc., and NIST urged organizations worldwide to **begin migrating to these quantum-resistant algorithms** as soon as possible【57:7†source】,【57:7†source】. This call to action is motivated by the “harvest now, decrypt later” threat – adversaries could steal encrypted data today and decrypt it in the future once a powerful quantum computer (often termed a “**CRQC**” – Cryptographically Relevant Quantum Computer) becomes available【57:7†source】,【57:7†source】. Governments and industries have heeded the warning: the U.S. federal agencies, per mandate, started the transition to PQC in 2024, and banks and healthcare companies have initiated audits of their encryption schemes. In tandem, some progress in **quantum cryptography** (using quantum physics itself for security) was reported – for example, **quantum key distribution (QKD)** networks expanded in China and Europe, and a U.S. startup delivered a commercial entangled photon source for secure communications in mid-2025【57:6†source】. Researchers also demonstrated a “quantum safe” blockchain prototype using PQC algorithms. In summary, the past year saw **defensive innovations** to prepare cybersecurity for the quantum era, ensuring that as quantum computers grow more powerful, our sensitive information remains protected.

- **Emerging AI and ML Applications:** The intersection of quantum computing and artificial intelligence grew as a research theme. While still largely experimental, there were attempts to use quantum processors for *machine learning tasks* – such as classification, clustering, and generative modeling. Small-scale demonstrations showed that quantum kernel methods could classify certain datasets as accurately as classical SVMs, with the potential for improvement as qubit counts increase. Additionally, frameworks that integrate quantum circuits into machine learning pipelines matured. For instance, the **TorchQC library** (built on PyTorch) allows quantum dynamics simulations to be embedded in deep learning workflows. This enables using gradient-descent on quantum circuit parameters or applying reinforcement learning to optimize quantum control pulses. Such tools were used to train quantum models for simple pattern recognition and to optimize qubit calibration routines via AI. On the flip side, **quantum-inspired neural networks** (classical models inspired by quantum tensor networks) have started to outperform some traditional networks on specific data – hinting at a fertile cross-pollination between quantum physics and AI. Companies like Google and IBM are actively exploring **quantum neural network algorithms** that one day could run on quantum hardware to potentially accelerate training of AI models. While no clear quantum advantage in AI has been realized yet, the convergence of **quantum computing, AI, and high-performance computing** is a notable trend. Tech giants have begun referring to this trio as a future “quantum-centric supercomputing” paradigm【57:2†source】,【57:1†source】, where CPUs, GPUs, and QPUs (quantum processing units) work in concert. The past year’s developments – from error-corrected qubits to hybrid algorithms – are laying the groundwork for such integrated systems. Researchers and industry leaders alike believe that **quantum computing will eventually synergize with AI** to solve complex problems in drug design, climate modeling, and beyond, which are currently out of reach for either technology alone.

In summary, the last year has been **transformative for quantum computing**, with simultaneous progress on **hardware, algorithms, and real-world applications**. Academic groups published breakthrough results (like Google’s error-correction feat【57:3†source】 and novel algorithms【57:4†source】), while industry players delivered on ambitious roadmaps (IBM’s 156-qubit processor【57:9†source】, Microsoft’s topological qubit, IonQ’s scaling plans, etc.). These advances are steadily shifting quantum computing from a predominantly experimental field toward a more practical technology. Crucially, early **use-case demonstrations** – solving chemistry problems, speeding up engineering simulations, optimizing portfolios – hint at the broad impact quantum computers will have as they continue to improve. Challenges remain (scaling to thousands of logical qubits, further reducing error rates, and training a quantum-ready workforce), and some experts urge caution on timelines【57:5†source】.  Nonetheless, the **rapid innovation across superconducting, ion-trap, photonic, and other platforms** over the past year has given the community a surge of optimism. Quantum computing is entering a new stage where *useful quantum advantage* in specific domains is on the horizon. With tech giants and startups alike achieving record performance and governments standardizing for a post-quantum world, the period from mid-2024 to mid-2025 will be remembered as a tipping point – one that brought us significantly closer to the long-envisioned quantum revolution in computation. 

**Sources:**

1. IBM News Release – *IBM Quantum Developer Conference 2024 Highlights*【57:9†source】,【57:9†source】  
2. Google AI Blog – *Introducing the Willow Quantum Processor*【57:3†source】,【57:3†source】  
3. Microsoft Blog – *Reliable Logical Qubits Demo (Microsoft + Quantinuum)*【57:1†source】,【57:1†source】  
4. Forbes – *Pivotal Quantum Breakthroughs by Microsoft, Google, IBM (2025)*【57:5†source】,【57:5†source】  
5. Quantum Computing Report – *IonQ/Ansys Fluid Dynamics Simulation Speedup*,  
6. HPCwire – *NIST Post-Quantum Cryptography Standards Announcement*【57:7†source】,【57:7†source】  
7. Open Access Government – *Quantum Computing 2024 Overview (Hardware, Algorithms, Applications)*,  
8. Live Science – *Quantinuum 56-Qubit Error-Corrected Quantum Computer Record*,  
9. Marin Ivezic (PostQuantum) – *IonQ 2025 Roadmap and Industry Roadmaps*,  
10. Nature/Physics News – *Google’s Below-Threshold Error Correction Achievement*,【57:2†source】

## References
- [IBM - Image Gallery](https://newsroom.ibm.com/media-quantum-innovation?keywords=quantum&l=100)
- [Here are the latest weekly updates (June 2025) in the field of quantum ...](https://www.quantumhorizon.it/2025/06/21/here-are-the-latest-weekly-updates-june-2025-in-the-field-of-quantum-computing/)
- [Meet Willow, our state-of-the-art quantum chip - The Keyword](https://blog.google/technology/research/google-willow-quantum-chip/)
- [Advancing science: Microsoft and Quantinuum demonstrate the most ...](https://blogs.microsoft.com/blog/2024/04/03/advancing-science-microsoft-and-quantinuum-demonstrate-the-most-reliable-logical-qubits-on-record-with-an-error-rate-800x-better-than-physical-qubits/)
- [Recent Breakthroughs Accelerate The Race For Quantum Computing - Forbes](https://www.forbes.com/sites/drektadang/2025/03/09/recent-breakthroughs-accelerate-the-race-for-quantum-computing/)
- [The latest developments in quantum computing: A transformative frontier ...](https://www.openaccessgovernment.org/the-latest-developments-in-quantum-computing-a-transformative-frontier/187748/)
- [IBM Launches Its Most Advanced Quantum Computers, Fueling New ...](https://newsroom.ibm.com/2024-11-13-ibm-launches-its-most-advanced-quantum-computers,-fueling-new-scientific-value-and-progress-towards-quantum-advantage)
- [Top quantum algorithms papers — Winter 2024 edition - PennyLane Blog](https://pennylane.ai/blog/2024/04/top_quantum_algorithms_papers_winter_2024)
