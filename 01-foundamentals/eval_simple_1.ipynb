{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fb5c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment and authentication OK\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# load environment variables from .env file\n",
    "load_dotenv(dotenv_path=\".env\", override=True)\n",
    "\n",
    "from utils.auth import AuthHelper\n",
    "settings = AuthHelper.load_settings()\n",
    "credential = AuthHelper.test_credential()\n",
    "\n",
    "if credential:\n",
    "    print('Environment and authentication OK')\n",
    "else:\n",
    "    print(\"please login first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd68ea3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created agent, agent ID: asst_yDQOgHuLYD5zhwoebWPPunL7\n",
      "Created thread, thread ID: thread_vy65J3wNJgTOPTfs1cTauEqr\n",
      "Created message, message ID: msg_CLdpk67ZhwE0zX2MQjXVUODW\n",
      "Run status: RunStatus.COMPLETED\n"
     ]
    }
   ],
   "source": [
    "import os, time\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.ai.projects.models import (\n",
    "    AgentEvaluationRequest,\n",
    "    InputDataset,\n",
    "    EvaluatorIds,\n",
    "    EvaluatorConfiguration,\n",
    "    AgentEvaluationSamplingConfiguration,\n",
    "    AgentEvaluationRedactionConfiguration,\n",
    ")\n",
    "\n",
    "# not using the settings.agent_name, since this agent name is used by other notebooks\n",
    "AGENT_NAME = \"my-eval-agent-1\"\n",
    "AGENT_INSTRUCTIONS = \"You are helpful agent\"\n",
    "\n",
    "project_client = AIProjectClient(\n",
    "    endpoint=settings.project_endpoint, \n",
    "    credential=credential, \n",
    "#   api_version=settings.project_api_version\n",
    ")\n",
    "\n",
    "# [START evaluations_agent_sample]\n",
    "agent = project_client.agents.create_agent(\n",
    "    model=settings.model_deployment_name,\n",
    "    name=AGENT_NAME,\n",
    "    instructions=AGENT_INSTRUCTIONS,\n",
    ")\n",
    "print(f\"Created agent, agent ID: {agent.id}\")\n",
    "\n",
    "thread = project_client.agents.threads.create()\n",
    "print(f\"Created thread, thread ID: {thread.id}\")\n",
    "\n",
    "message = project_client.agents.messages.create(\n",
    "    thread_id=thread.id, role=\"user\", content=\"Hello, tell me a joke\"\n",
    ")\n",
    "print(f\"Created message, message ID: {message.id}\")\n",
    "\n",
    "run = project_client.agents.runs.create(thread_id=thread.id, agent_id=agent.id)\n",
    "\n",
    "# Poll the run as long as run status is queued or in progress\n",
    "while run.status in [\"queued\", \"in_progress\", \"requires_action\"]:\n",
    "    # Wait for a second\n",
    "    time.sleep(1)\n",
    "    run = project_client.agents.runs.get(thread_id=thread.id, run_id=run.id)\n",
    "    print(f\"Run status: {run.status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "786148db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'thread_vy65J3wNJgTOPTfs1cTauEqr;run_Pt8pHZe7g96JBV89nWwuKKcf', 'status': 'Running', 'result': None, 'error': None}\n"
     ]
    }
   ],
   "source": [
    "agent_evaluation_request = AgentEvaluationRequest(\n",
    "    run_id=run.id,\n",
    "    thread_id=thread.id,\n",
    "    evaluators={\n",
    "        \"violence\": EvaluatorConfiguration(\n",
    "            id=EvaluatorIds.VIOLENCE,\n",
    "        )\n",
    "    },\n",
    "    sampling_configuration=AgentEvaluationSamplingConfiguration(\n",
    "        name=\"test\",\n",
    "        sampling_percent=100,\n",
    "        max_request_rate=100,\n",
    "    ),\n",
    "    redaction_configuration=AgentEvaluationRedactionConfiguration(\n",
    "        redact_score_properties=False,\n",
    "    ),\n",
    "    app_insights_connection_string=project_client.telemetry.get_connection_string(),\n",
    ")\n",
    "\n",
    "agent_evaluation_response = project_client.evaluations.create_agent_evaluation(\n",
    "    evaluation=agent_evaluation_request\n",
    ")\n",
    "\n",
    "print(agent_evaluation_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f40fbc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azmultiagents3.12pip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
